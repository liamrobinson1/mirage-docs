
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/01-light_curves/neural_network_brightness.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_01-light_curves_neural_network_brightness.py:


Neural Network Brightness
=========================

Trains a neural network to predict the brightness of a specular cube in an arbitrary lighting and observation conditions and compares the results to the truth

.. GENERATED FROM PYTHON SOURCE LINES 7-19

.. code-block:: default



    import sys

    sys.path.append(".")

    import numpy as np
    import pyspaceaware as ps
    import pyspaceaware.sim as pssim
    import matplotlib.pyplot as plt
    import seaborn as sns








.. GENERATED FROM PYTHON SOURCE LINES 20-21

Let's define the object and the BRDF

.. GENERATED FROM PYTHON SOURCE LINES 21-23

.. code-block:: default

    obj = ps.SpaceObject("cube.obj")
    brdf = ps.Brdf("phong", cd=0.5, cs=0.5, n=10)







.. GENERATED FROM PYTHON SOURCE LINES 24-25

We now define the Multi-Layer Perceptron (MLP) brightness model. Note that the ``layers=(150, 50, 150)`` keyword argument defines the number of neurons in each densely-connected layer.

.. GENERATED FROM PYTHON SOURCE LINES 25-26

.. code-block:: default

    mlp_bm = pssim.MLPBrightnessModel(obj, brdf, use_engine=False, train_on="irradiance")







.. GENERATED FROM PYTHON SOURCE LINES 27-28

Now we train the model on a set number of training lighting and observation configurations. Usually ``1e5``-``1e6`` are required for a *good* fit

.. GENERATED FROM PYTHON SOURCE LINES 28-31

.. code-block:: default

    num_train = int(1e3)
    mlp_bm.train(num_train)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 1.43e-03 seconds
    (1024, 6) (1024,)
    Iteration 1, loss = 0.21730991
    Iteration 2, loss = 0.14876936
    Iteration 3, loss = 0.14398718
    Iteration 4, loss = 0.13357468
    Iteration 5, loss = 0.13157055
    Iteration 6, loss = 0.12726204
    Iteration 7, loss = 0.12357188
    Iteration 8, loss = 0.12121558
    Iteration 9, loss = 0.11881827
    Iteration 10, loss = 0.11686588
    Iteration 11, loss = 0.11460711
    Iteration 12, loss = 0.11256216
    Iteration 13, loss = 0.11082868
    Iteration 14, loss = 0.10992303
    Iteration 15, loss = 0.10778232
    Iteration 16, loss = 0.10519939
    Iteration 17, loss = 0.10421909
    Iteration 18, loss = 0.10126227
    Iteration 19, loss = 0.10009438
    Iteration 20, loss = 0.09799114
    Iteration 21, loss = 0.09545334
    Iteration 22, loss = 0.09245845
    Iteration 23, loss = 0.09013639
    Iteration 24, loss = 0.08671801
    Iteration 25, loss = 0.08430875
    Iteration 26, loss = 0.08134792
    Iteration 27, loss = 0.07796199
    Iteration 28, loss = 0.07476226
    Iteration 29, loss = 0.07183075
    Iteration 30, loss = 0.06863358
    Iteration 31, loss = 0.06633702
    Iteration 32, loss = 0.06407647
    Iteration 33, loss = 0.06023889
    Iteration 34, loss = 0.05598674
    Iteration 35, loss = 0.05602391
    Iteration 36, loss = 0.05349043
    Iteration 37, loss = 0.05280932
    Iteration 38, loss = 0.04897785
    Iteration 39, loss = 0.04832561
    Iteration 40, loss = 0.04675550
    Iteration 41, loss = 0.04696855
    Iteration 42, loss = 0.04495740
    Iteration 43, loss = 0.04225347
    Iteration 44, loss = 0.04011199
    Iteration 45, loss = 0.03847036
    Iteration 46, loss = 0.03756012
    Iteration 47, loss = 0.03921331
    Iteration 48, loss = 0.03701061
    Iteration 49, loss = 0.03390465
    Iteration 50, loss = 0.03429532
    Iteration 51, loss = 0.03223207
    Iteration 52, loss = 0.03184851
    Iteration 53, loss = 0.02987602
    Iteration 54, loss = 0.02882419
    Iteration 55, loss = 0.02855139
    Iteration 56, loss = 0.02722365
    Iteration 57, loss = 0.02640586
    Iteration 58, loss = 0.02672861
    Iteration 59, loss = 0.02742103
    Iteration 60, loss = 0.02499921
    Iteration 61, loss = 0.02506003
    Iteration 62, loss = 0.02424745
    Iteration 63, loss = 0.02276821
    Iteration 64, loss = 0.02218271
    Iteration 65, loss = 0.02170531
    Iteration 66, loss = 0.02129659
    Iteration 67, loss = 0.02027375
    Iteration 68, loss = 0.01830815
    Iteration 69, loss = 0.01754127
    Iteration 70, loss = 0.01642109
    Iteration 71, loss = 0.01592904
    Iteration 72, loss = 0.01563968
    Iteration 73, loss = 0.01567486
    Iteration 74, loss = 0.01614228
    Iteration 75, loss = 0.01486425
    Iteration 76, loss = 0.01350069
    Iteration 77, loss = 0.01298017
    Iteration 78, loss = 0.01350548
    Iteration 79, loss = 0.01272697
    Iteration 80, loss = 0.01221966
    Iteration 81, loss = 0.01166421
    Iteration 82, loss = 0.01225829
    Iteration 83, loss = 0.01155089
    Iteration 84, loss = 0.01073415
    Iteration 85, loss = 0.01035461
    Iteration 86, loss = 0.01001118
    Iteration 87, loss = 0.01004854
    Iteration 88, loss = 0.00899560
    Iteration 89, loss = 0.00967165
    Iteration 90, loss = 0.00904344
    Iteration 91, loss = 0.00875785
    Iteration 92, loss = 0.00812705
    Iteration 93, loss = 0.00775716
    Iteration 94, loss = 0.00758263
    Iteration 95, loss = 0.00778722
    Iteration 96, loss = 0.00901583
    Iteration 97, loss = 0.00811392
    Iteration 98, loss = 0.00896824
    Iteration 99, loss = 0.00733221
    Iteration 100, loss = 0.00737176
    Iteration 101, loss = 0.00729931
    Iteration 102, loss = 0.00619830
    Iteration 103, loss = 0.00613130
    Iteration 104, loss = 0.00610472
    Iteration 105, loss = 0.00634081
    Iteration 106, loss = 0.00676747
    Iteration 107, loss = 0.00657195
    Iteration 108, loss = 0.00690978
    Iteration 109, loss = 0.00672123
    Iteration 110, loss = 0.00646027
    Iteration 111, loss = 0.00654013
    Iteration 112, loss = 0.00618483
    Iteration 113, loss = 0.00544820
    Iteration 114, loss = 0.00522224
    Iteration 115, loss = 0.00495776
    Iteration 116, loss = 0.00542503
    Iteration 117, loss = 0.00505151
    Iteration 118, loss = 0.00497871
    Iteration 119, loss = 0.00453419
    Iteration 120, loss = 0.00440115
    Iteration 121, loss = 0.00436155
    Iteration 122, loss = 0.00496169
    Iteration 123, loss = 0.00402377
    Iteration 124, loss = 0.00409787
    Iteration 125, loss = 0.00350602
    Iteration 126, loss = 0.00361215
    Iteration 127, loss = 0.00347769
    Iteration 128, loss = 0.00320962
    Iteration 129, loss = 0.00303993
    Iteration 130, loss = 0.00285875
    Iteration 131, loss = 0.00294981
    Iteration 132, loss = 0.00277158
    Iteration 133, loss = 0.00257138
    Iteration 134, loss = 0.00297222
    Iteration 135, loss = 0.00288982
    Iteration 136, loss = 0.00330800
    Iteration 137, loss = 0.00301878
    Iteration 138, loss = 0.00301793
    Iteration 139, loss = 0.00263233
    Iteration 140, loss = 0.00264610
    Iteration 141, loss = 0.00241600
    Iteration 142, loss = 0.00227487
    Iteration 143, loss = 0.00212176
    Iteration 144, loss = 0.00210638
    Iteration 145, loss = 0.00218358
    Iteration 146, loss = 0.00238438
    Iteration 147, loss = 0.00240726
    Iteration 148, loss = 0.00247091
    Iteration 149, loss = 0.00234643
    Iteration 150, loss = 0.00233968
    Iteration 151, loss = 0.00223766
    Iteration 152, loss = 0.00206906
    Iteration 153, loss = 0.00201685
    Iteration 154, loss = 0.00213588
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 8.85e+00 seconds
    Serialize model: 6.43e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 32-33

We can now simulate a torque-free attitude profile to inspect the quality of the fit

.. GENERATED FROM PYTHON SOURCE LINES 33-44

.. code-block:: default

    t_eval = np.linspace(0, 10, 1000)
    q, _ = ps.propagate_attitude_torque_free(
        np.array([0.0, 0.0, 0.0, 1.0]),
        np.array([1.0, 1.0, 1.0]),
        np.diag([1, 2, 3]),
        t_eval,
    )
    dcm = ps.quat_to_dcm(q)
    ovb = ps.stack_mat_mult_vec(dcm, np.array([[1, 0, 0]]))
    svb = ps.stack_mat_mult_vec(dcm, np.array([[0, 1, 0]]))








.. GENERATED FROM PYTHON SOURCE LINES 45-46

Evaluating the model in its two available formats - as a native ``scikit-learn`` model and as an Open Neural Network eXchange (ONNX) model

.. GENERATED FROM PYTHON SOURCE LINES 46-53

.. code-block:: default

    ps.tic("Evaluate trained model with sklearn")
    mdl_b_sklearn = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    ps.toc()
    ps.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    ps.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate trained model with sklearn: 3.77e-02 seconds
    Evaluate trained model with onnx: 3.29e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 54-55

We can save both of these representations to file:

.. GENERATED FROM PYTHON SOURCE LINES 55-58

.. code-block:: default

    mlp_bm.save_to_file(save_as_format="onnx")
    mlp_bm.save_to_file(save_as_format="sklearn")








.. GENERATED FROM PYTHON SOURCE LINES 59-60

Now we load the model from its ``.onxx`` file we just saved and evaluate the brightness

.. GENERATED FROM PYTHON SOURCE LINES 60-65

.. code-block:: default

    mlp_bm.load_from_file(mlp_bm.onnx_file_name)
    ps.tic("Evaluate loaded model with onxx")
    mdl_onnx_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    ps.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with onxx: 2.31e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 66-67

And we do the same for the ``scikit-learn`` ``.plk`` file we saved

.. GENERATED FROM PYTHON SOURCE LINES 67-72

.. code-block:: default

    mlp_bm.load_from_file(mlp_bm.sklearn_file_name)
    ps.tic("Evaluate loaded model with sklearn")
    mdl_sklearn_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    ps.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with sklearn: 1.11e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 73-74

We can easily confirm that all four model evaluations have produced the same prediction

.. GENERATED FROM PYTHON SOURCE LINES 74-79

.. code-block:: default

    print(np.max(np.abs(mdl_b_sklearn - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_sklearn - mdl_sklearn_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_sklearn_loaded)))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    8.694658004682054e-07
    0.0
    0.0
    8.694658004682054e-07




.. GENERATED FROM PYTHON SOURCE LINES 80-81

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 81-93

.. code-block:: default

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_sklearn, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {num_train} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Normalized brightness")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()




.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 94-95

We can also train on magnitude data instead of irradiance:

.. GENERATED FROM PYTHON SOURCE LINES 95-102

.. code-block:: default

    mlp_bm = pssim.MLPBrightnessModel(obj, brdf, use_engine=True, train_on="magnitude")
    mlp_bm.train(num_train)

    ps.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb)
    ps.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Running Light Curve Engine: 
    /Users/liamrobinson/Documents/PyLightCurves/pyspaceaware/ctools/engine/LightCurveEngine-mac-arm -m /Users/liamrobinson/Documents/PyLightCurves/pyspaceaware/resources/models/cube.obj -i 9 -c 1024 -r light_curve0.lcr -x /Users/liamrobinson/Documents/PyLightCurves/examples/01-light_curves/out  -b 1 -D 0.5 -S 0.5 -N 10

    Compute training LC: 6.68e-01 seconds
    (1024, 6) (1024,)
    Iteration 1, loss = 138.81149237
    Iteration 2, loss = 127.97017241
    Iteration 3, loss = 112.42881212
    Iteration 4, loss = 88.43753867
    Iteration 5, loss = 55.46285934
    Iteration 6, loss = 22.48737453
    Iteration 7, loss = 10.51218663
    Iteration 8, loss = 13.77269399
    Iteration 9, loss = 8.59426332
    Iteration 10, loss = 7.69308824
    Iteration 11, loss = 7.85473596
    Iteration 12, loss = 6.75538975
    Iteration 13, loss = 6.40802172
    Iteration 14, loss = 6.26429227
    Iteration 15, loss = 5.90351362
    Iteration 16, loss = 5.82169103
    Iteration 17, loss = 5.69910369
    Iteration 18, loss = 5.51884410
    Iteration 19, loss = 5.42495158
    Iteration 20, loss = 5.32787792
    Iteration 21, loss = 5.25188038
    Iteration 22, loss = 5.22924505
    Iteration 23, loss = 5.17575306
    Iteration 24, loss = 5.12208188
    Iteration 25, loss = 5.07660298
    Iteration 26, loss = 5.03847163
    Iteration 27, loss = 5.00421095
    Iteration 28, loss = 4.97612448
    Iteration 29, loss = 4.94784101
    Iteration 30, loss = 4.92409229
    Iteration 31, loss = 4.92195070
    Iteration 32, loss = 4.89923282
    Iteration 33, loss = 4.87204307
    Iteration 34, loss = 4.85704521
    Iteration 35, loss = 4.82359105
    Iteration 36, loss = 4.81727855
    Iteration 37, loss = 4.79693905
    Iteration 38, loss = 4.78139484
    Iteration 39, loss = 4.76538886
    Iteration 40, loss = 4.73188291
    Iteration 41, loss = 4.70943695
    Iteration 42, loss = 4.69136771
    Iteration 43, loss = 4.67366834
    Iteration 44, loss = 4.65887317
    Iteration 45, loss = 4.64135520
    Iteration 46, loss = 4.61483100
    Iteration 47, loss = 4.61004653
    Iteration 48, loss = 4.59918552
    Iteration 49, loss = 4.59680219
    Iteration 50, loss = 4.56820279
    Iteration 51, loss = 4.55341406
    Iteration 52, loss = 4.54182221
    Iteration 53, loss = 4.52336682
    Iteration 54, loss = 4.50086956
    Iteration 55, loss = 4.48703898
    Iteration 56, loss = 4.47216154
    Iteration 57, loss = 4.49184207
    Iteration 58, loss = 4.46019516
    Iteration 59, loss = 4.44875251
    Iteration 60, loss = 4.40866464
    Iteration 61, loss = 4.40418312
    Iteration 62, loss = 4.38074726
    Iteration 63, loss = 4.36970347
    Iteration 64, loss = 4.34864823
    Iteration 65, loss = 4.34316955
    Iteration 66, loss = 4.32615105
    Iteration 67, loss = 4.30619321
    Iteration 68, loss = 4.30425041
    Iteration 69, loss = 4.30642299
    Iteration 70, loss = 4.27974145
    Iteration 71, loss = 4.26892121
    Iteration 72, loss = 4.25081979
    Iteration 73, loss = 4.23866820
    Iteration 74, loss = 4.22724994
    Iteration 75, loss = 4.21803572
    Iteration 76, loss = 4.20883289
    Iteration 77, loss = 4.18057958
    Iteration 78, loss = 4.17234048
    Iteration 79, loss = 4.17357802
    Iteration 80, loss = 4.15433042
    Iteration 81, loss = 4.13804032
    Iteration 82, loss = 4.12970886
    Iteration 83, loss = 4.11387526
    Iteration 84, loss = 4.08704381
    Iteration 85, loss = 4.09535755
    Iteration 86, loss = 4.06754718
    Iteration 87, loss = 4.07027193
    Iteration 88, loss = 4.05835323
    Iteration 89, loss = 4.03191162
    Iteration 90, loss = 4.02694360
    Iteration 91, loss = 3.99647822
    Iteration 92, loss = 4.00717813
    Iteration 93, loss = 3.99271594
    Iteration 94, loss = 3.96506111
    Iteration 95, loss = 3.96821999
    Iteration 96, loss = 3.94141571
    Iteration 97, loss = 3.93062885
    Iteration 98, loss = 3.92142491
    Iteration 99, loss = 3.90299886
    Iteration 100, loss = 3.89075299
    Iteration 101, loss = 3.88333808
    Iteration 102, loss = 3.86525182
    Iteration 103, loss = 3.86574469
    Iteration 104, loss = 3.87023114
    Iteration 105, loss = 3.88652102
    Iteration 106, loss = 3.87839052
    Iteration 107, loss = 3.83449353
    Iteration 108, loss = 3.79256043
    Iteration 109, loss = 3.77014619
    Iteration 110, loss = 3.76986358
    Iteration 111, loss = 3.77605595
    Iteration 112, loss = 3.74439213
    Iteration 113, loss = 3.70928402
    Iteration 114, loss = 3.69928557
    Iteration 115, loss = 3.72654619
    Iteration 116, loss = 3.70005826
    Iteration 117, loss = 3.72423108
    Iteration 118, loss = 3.68510218
    Iteration 119, loss = 3.67047245
    Iteration 120, loss = 3.62142634
    Iteration 121, loss = 3.64997018
    Iteration 122, loss = 3.59220640
    Iteration 123, loss = 3.57588636
    Iteration 124, loss = 3.58201515
    Iteration 125, loss = 3.53451262
    Iteration 126, loss = 3.52145132
    Iteration 127, loss = 3.48756125
    Iteration 128, loss = 3.46337065
    Iteration 129, loss = 3.45529823
    Iteration 130, loss = 3.44286287
    Iteration 131, loss = 3.44166844
    Iteration 132, loss = 3.52351919
    Iteration 133, loss = 3.44621454
    Iteration 134, loss = 3.39299875
    Iteration 135, loss = 3.37279934
    Iteration 136, loss = 3.38169067
    Iteration 137, loss = 3.34347779
    Iteration 138, loss = 3.32492818
    Iteration 139, loss = 3.37283143
    Iteration 140, loss = 3.27928286
    Iteration 141, loss = 3.29569088
    Iteration 142, loss = 3.29247035
    Iteration 143, loss = 3.30691405
    Iteration 144, loss = 3.18622147
    Iteration 145, loss = 3.16233848
    Iteration 146, loss = 3.15869323
    Iteration 147, loss = 3.13851925
    Iteration 148, loss = 3.12534853
    Iteration 149, loss = 3.05134044
    Iteration 150, loss = 3.05902842
    Iteration 151, loss = 3.10662139
    Iteration 152, loss = 3.11624764
    Iteration 153, loss = 3.13896444
    Iteration 154, loss = 3.05898630
    Iteration 155, loss = 2.94079872
    Iteration 156, loss = 2.88423915
    Iteration 157, loss = 2.90531828
    Iteration 158, loss = 2.87241475
    Iteration 159, loss = 2.86598349
    Iteration 160, loss = 2.84334334
    Iteration 161, loss = 2.83569984
    Iteration 162, loss = 2.83410066
    Iteration 163, loss = 2.79504567
    Iteration 164, loss = 2.86281166
    Iteration 165, loss = 2.79732526
    Iteration 166, loss = 2.70995839
    Iteration 167, loss = 2.69849808
    Iteration 168, loss = 2.69752309
    Iteration 169, loss = 2.73271156
    Iteration 170, loss = 2.69123867
    Iteration 171, loss = 2.62678134
    Iteration 172, loss = 2.62571034
    Iteration 173, loss = 2.59969161
    Iteration 174, loss = 2.59463812
    Iteration 175, loss = 2.55382348
    Iteration 176, loss = 2.54476489
    Iteration 177, loss = 2.52102101
    Iteration 178, loss = 2.60115497
    Iteration 179, loss = 2.48428272
    Iteration 180, loss = 2.45044132
    Iteration 181, loss = 2.43673587
    Iteration 182, loss = 2.44016360
    Iteration 183, loss = 2.48914626
    Iteration 184, loss = 2.46065623
    Iteration 185, loss = 2.41381064
    Iteration 186, loss = 2.35585842
    Iteration 187, loss = 2.30774038
    Iteration 188, loss = 2.30061610
    Iteration 189, loss = 2.28280900
    Iteration 190, loss = 2.29242254
    Iteration 191, loss = 2.27312844
    Iteration 192, loss = 2.27475703
    Iteration 193, loss = 2.27697372
    Iteration 194, loss = 2.21462695
    Iteration 195, loss = 2.20182342
    Iteration 196, loss = 2.18137911
    Iteration 197, loss = 2.16832936
    Iteration 198, loss = 2.17356297
    Iteration 199, loss = 2.14399957
    Iteration 200, loss = 2.12093330
    /Users/liamrobinson/Documents/PyLightCurves/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
      warnings.warn(
    Fit against 1000 pts: : 1.32e+01 seconds
    Serialize model: 3.45e-02 seconds
    Evaluate trained model with onnx: 4.43e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 103-104

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 104-115

.. code-block:: default

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_onnx, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {int(1e6)} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Apparent Magnitude")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()



.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png
   :alt: Light Curves for cube.obj, 1000000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Running Light Curve Engine: 
    /Users/liamrobinson/Documents/PyLightCurves/pyspaceaware/ctools/engine/LightCurveEngine-mac-arm -m /Users/liamrobinson/Documents/PyLightCurves/pyspaceaware/resources/models/cube.obj -i 9 -c 1000 -r light_curve0.lcr -x /Users/liamrobinson/Documents/PyLightCurves/examples/01-light_curves/out  -b 1 -D 0.5 -S 0.5 -N 10






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  24.016 seconds)


.. _sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: neural_network_brightness.py <neural_network_brightness.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: neural_network_brightness.ipynb <neural_network_brightness.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
