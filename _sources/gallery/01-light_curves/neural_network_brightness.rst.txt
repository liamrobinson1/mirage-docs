
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/01-light_curves/neural_network_brightness.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_01-light_curves_neural_network_brightness.py:


Neural Network Brightness
=========================

Trains a neural network to predict the brightness of a specular cube in an arbitrary lighting and observation conditions and compares the results to the truth

.. GENERATED FROM PYTHON SOURCE LINES 7-15

.. code-block:: Python


    import matplotlib.pyplot as plt
    import numpy as np
    import seaborn as sns

    import mirage as mr
    import mirage.sim as mrsim








.. GENERATED FROM PYTHON SOURCE LINES 16-17

Let's define the object and the BRDF

.. GENERATED FROM PYTHON SOURCE LINES 17-19

.. code-block:: Python

    obj = mr.SpaceObject("cube.obj")
    brdf = mr.Brdf("phong", cd=0.5, cs=0.5, n=10)







.. GENERATED FROM PYTHON SOURCE LINES 20-21

We now define the Multi-Layer Perceptron (MLP) brightness model. Note that the ``layers=(150, 50, 150)`` keyword argument defines the number of neurons in each densely-connected layer.

.. GENERATED FROM PYTHON SOURCE LINES 21-22

.. code-block:: Python

    mlp_bm = mrsim.MLPBrightnessModel(obj, brdf, use_engine=False)







.. GENERATED FROM PYTHON SOURCE LINES 23-24

Now we train the model on a set number of training lighting and observation configurations. Usually ``1e5``-``1e6`` are required for a *good* fit

.. GENERATED FROM PYTHON SOURCE LINES 24-27

.. code-block:: Python

    num_train = int(1e3)
    mlp_bm.train(num_train)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 2.56e-03 seconds
    Iteration 1, loss = 0.20516094
    Iteration 2, loss = 0.16178311
    Iteration 3, loss = 0.14698173
    Iteration 4, loss = 0.14534670
    Iteration 5, loss = 0.14228715
    Iteration 6, loss = 0.14055198
    Iteration 7, loss = 0.13944917
    Iteration 8, loss = 0.13800653
    Iteration 9, loss = 0.13650523
    Iteration 10, loss = 0.13516938
    Iteration 11, loss = 0.13331286
    Iteration 12, loss = 0.13259166
    Iteration 13, loss = 0.13073843
    Iteration 14, loss = 0.12866297
    Iteration 15, loss = 0.12679027
    Iteration 16, loss = 0.12546357
    Iteration 17, loss = 0.12162890
    Iteration 18, loss = 0.11922875
    Iteration 19, loss = 0.11639264
    Iteration 20, loss = 0.11329479
    Iteration 21, loss = 0.10828122
    Iteration 22, loss = 0.10571689
    Iteration 23, loss = 0.10182985
    Iteration 24, loss = 0.09660230
    Iteration 25, loss = 0.09112805
    Iteration 26, loss = 0.08497967
    Iteration 27, loss = 0.07968892
    Iteration 28, loss = 0.07563348
    Iteration 29, loss = 0.07143320
    Iteration 30, loss = 0.06468808
    Iteration 31, loss = 0.06324266
    Iteration 32, loss = 0.05808301
    Iteration 33, loss = 0.05711298
    Iteration 34, loss = 0.05349126
    Iteration 35, loss = 0.05231532
    Iteration 36, loss = 0.05526093
    Iteration 37, loss = 0.04986449
    Iteration 38, loss = 0.04309527
    Iteration 39, loss = 0.04173356
    Iteration 40, loss = 0.04144343
    Iteration 41, loss = 0.04014250
    Iteration 42, loss = 0.03678040
    Iteration 43, loss = 0.03447400
    Iteration 44, loss = 0.03506349
    Iteration 45, loss = 0.03451986
    Iteration 46, loss = 0.03114459
    Iteration 47, loss = 0.03131672
    Iteration 48, loss = 0.02933001
    Iteration 49, loss = 0.02954514
    Iteration 50, loss = 0.02778838
    Iteration 51, loss = 0.02741257
    Iteration 52, loss = 0.02515441
    Iteration 53, loss = 0.02631921
    Iteration 54, loss = 0.02576581
    Iteration 55, loss = 0.02292156
    Iteration 56, loss = 0.02360873
    Iteration 57, loss = 0.02532310
    Iteration 58, loss = 0.02441176
    Iteration 59, loss = 0.02568687
    Iteration 60, loss = 0.02268060
    Iteration 61, loss = 0.02002593
    Iteration 62, loss = 0.01909350
    Iteration 63, loss = 0.01738442
    Iteration 64, loss = 0.01711152
    Iteration 65, loss = 0.01713526
    Iteration 66, loss = 0.01590242
    Iteration 67, loss = 0.01627049
    Iteration 68, loss = 0.01595949
    Iteration 69, loss = 0.01504803
    Iteration 70, loss = 0.01340045
    Iteration 71, loss = 0.01257193
    Iteration 72, loss = 0.01243004
    Iteration 73, loss = 0.01178436
    Iteration 74, loss = 0.01174630
    Iteration 75, loss = 0.01187018
    Iteration 76, loss = 0.01077362
    Iteration 77, loss = 0.01100523
    Iteration 78, loss = 0.01049954
    Iteration 79, loss = 0.01005060
    Iteration 80, loss = 0.01006047
    Iteration 81, loss = 0.01084043
    Iteration 82, loss = 0.01002028
    Iteration 83, loss = 0.00922606
    Iteration 84, loss = 0.00893765
    Iteration 85, loss = 0.00893372
    Iteration 86, loss = 0.00794708
    Iteration 87, loss = 0.00751390
    Iteration 88, loss = 0.00744821
    Iteration 89, loss = 0.00817370
    Iteration 90, loss = 0.00785172
    Iteration 91, loss = 0.00749354
    Iteration 92, loss = 0.00687406
    Iteration 93, loss = 0.00653474
    Iteration 94, loss = 0.00631266
    Iteration 95, loss = 0.00661939
    Iteration 96, loss = 0.00701104
    Iteration 97, loss = 0.00736738
    Iteration 98, loss = 0.00687014
    Iteration 99, loss = 0.00623806
    Iteration 100, loss = 0.00599546
    Iteration 101, loss = 0.00603957
    Iteration 102, loss = 0.00553842
    Iteration 103, loss = 0.00556606
    Iteration 104, loss = 0.00556472
    Iteration 105, loss = 0.00507029
    Iteration 106, loss = 0.00507220
    Iteration 107, loss = 0.00494330
    Iteration 108, loss = 0.00514644
    Iteration 109, loss = 0.00498520
    Iteration 110, loss = 0.00467556
    Iteration 111, loss = 0.00421088
    Iteration 112, loss = 0.00443678
    Iteration 113, loss = 0.00427211
    Iteration 114, loss = 0.00436954
    Iteration 115, loss = 0.00378674
    Iteration 116, loss = 0.00386643
    Iteration 117, loss = 0.00391969
    Iteration 118, loss = 0.00347525
    Iteration 119, loss = 0.00355261
    Iteration 120, loss = 0.00338486
    Iteration 121, loss = 0.00343809
    Iteration 122, loss = 0.00371126
    Iteration 123, loss = 0.00366757
    Iteration 124, loss = 0.00329894
    Iteration 125, loss = 0.00303093
    Iteration 126, loss = 0.00289026
    Iteration 127, loss = 0.00280705
    Iteration 128, loss = 0.00282640
    Iteration 129, loss = 0.00290152
    Iteration 130, loss = 0.00277365
    Iteration 131, loss = 0.00266232
    Iteration 132, loss = 0.00242983
    Iteration 133, loss = 0.00227380
    Iteration 134, loss = 0.00276358
    Iteration 135, loss = 0.00267445
    Iteration 136, loss = 0.00267046
    Iteration 137, loss = 0.00237533
    Iteration 138, loss = 0.00274114
    Iteration 139, loss = 0.00304208
    Iteration 140, loss = 0.00260834
    Iteration 141, loss = 0.00213429
    Iteration 142, loss = 0.00254024
    Iteration 143, loss = 0.00239574
    Iteration 144, loss = 0.00214277
    Iteration 145, loss = 0.00187413
    Iteration 146, loss = 0.00211917
    Iteration 147, loss = 0.00177001
    Iteration 148, loss = 0.00168905
    Iteration 149, loss = 0.00163365
    Iteration 150, loss = 0.00162221
    Iteration 151, loss = 0.00200623
    Iteration 152, loss = 0.00213801
    Iteration 153, loss = 0.00266905
    Iteration 154, loss = 0.00243497
    Iteration 155, loss = 0.00180272
    Iteration 156, loss = 0.00157432
    Iteration 157, loss = 0.00159976
    Iteration 158, loss = 0.00160144
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 2.94e+01 seconds




.. GENERATED FROM PYTHON SOURCE LINES 28-29

We can now simulate a torque-free attitude profile to inspect the quality of the fit

.. GENERATED FROM PYTHON SOURCE LINES 29-40

.. code-block:: Python

    t_eval = np.linspace(0, 10, 1000)
    q, _ = mr.propagate_attitude_torque_free(
        np.array([0.0, 0.0, 0.0, 1.0]),
        np.array([1.0, 1.0, 1.0]),
        np.diag([1, 2, 3]),
        t_eval,
    )
    dcm = mr.quat_to_dcm(q)
    ovb = mr.stack_mat_mult_vec(dcm, np.array([[1, 0, 0]]))
    svb = mr.stack_mat_mult_vec(dcm, np.array([[0, 1, 0]]))








.. GENERATED FROM PYTHON SOURCE LINES 41-42

Evaluating the model in its two available formats - as a native ``scikit-learn`` model and as an Open Neural Network eXchange (ONNX) model

.. GENERATED FROM PYTHON SOURCE LINES 42-49

.. code-block:: Python

    mr.tic("Evaluate trained model with sklearn")
    mdl_b_sklearn = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    mr.toc()
    mr.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate trained model with sklearn: 2.46e-02 seconds
    Evaluate trained model with onnx: 3.12e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 50-51

We can save both of these representations to file:

.. GENERATED FROM PYTHON SOURCE LINES 51-54

.. code-block:: Python

    mlp_bm.save_to_file(save_as_format="onnx")
    mlp_bm.save_to_file(save_as_format="sklearn")








.. GENERATED FROM PYTHON SOURCE LINES 55-56

Now we load the model from its ``.onxx`` file we just saved and evaluate the brightness

.. GENERATED FROM PYTHON SOURCE LINES 56-61

.. code-block:: Python

    mlp_bm.load_from_file(mlp_bm.onnx_file_name)
    mr.tic("Evaluate loaded model with onxx")
    mdl_onnx_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with onxx: 2.01e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 62-63

And we do the same for the ``scikit-learn`` ``.plk`` file we saved

.. GENERATED FROM PYTHON SOURCE LINES 63-68

.. code-block:: Python

    mlp_bm.load_from_file(mlp_bm.sklearn_file_name)
    mr.tic("Evaluate loaded model with sklearn")
    mdl_sklearn_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with sklearn: 9.19e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 69-70

We can easily confirm that all four model evaluations have produced the same prediction

.. GENERATED FROM PYTHON SOURCE LINES 70-75

.. code-block:: Python

    print(np.max(np.abs(mdl_b_sklearn - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_sklearn - mdl_sklearn_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_sklearn_loaded)))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    9.822406985549037e-07
    0.0
    0.0
    9.822406985549037e-07




.. GENERATED FROM PYTHON SOURCE LINES 76-77

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 77-89

.. code-block:: Python

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_sklearn, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {num_train} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Normalized brightness")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()




.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001_2_00x.png 2.00x
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 90-91

We can also train on magnitude data instead of irradiance:

.. GENERATED FROM PYTHON SOURCE LINES 91-98

.. code-block:: Python

    mlp_bm = mrsim.MLPBrightnessModel(obj, brdf, use_engine=True)
    mlp_bm.train(num_train)

    mr.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb)
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 8.63e-01 seconds
    Iteration 1, loss = 0.22191851
    Iteration 2, loss = 0.14613514
    Iteration 3, loss = 0.13364968
    Iteration 4, loss = 0.12526260
    Iteration 5, loss = 0.12308654
    Iteration 6, loss = 0.12132050
    Iteration 7, loss = 0.12123539
    Iteration 8, loss = 0.11973905
    Iteration 9, loss = 0.11767931
    Iteration 10, loss = 0.11727665
    Iteration 11, loss = 0.11601813
    Iteration 12, loss = 0.11442868
    Iteration 13, loss = 0.11229992
    Iteration 14, loss = 0.11242165
    Iteration 15, loss = 0.10983746
    Iteration 16, loss = 0.10755122
    Iteration 17, loss = 0.10593944
    Iteration 18, loss = 0.10411829
    Iteration 19, loss = 0.10168324
    Iteration 20, loss = 0.09964657
    Iteration 21, loss = 0.09608116
    Iteration 22, loss = 0.09268664
    Iteration 23, loss = 0.08902537
    Iteration 24, loss = 0.08601632
    Iteration 25, loss = 0.08089548
    Iteration 26, loss = 0.07689057
    Iteration 27, loss = 0.07166378
    Iteration 28, loss = 0.06773812
    Iteration 29, loss = 0.06297109
    Iteration 30, loss = 0.05913994
    Iteration 31, loss = 0.05406784
    Iteration 32, loss = 0.05149261
    Iteration 33, loss = 0.05016659
    Iteration 34, loss = 0.04832445
    Iteration 35, loss = 0.04839349
    Iteration 36, loss = 0.04282017
    Iteration 37, loss = 0.04069140
    Iteration 38, loss = 0.03785644
    Iteration 39, loss = 0.03762836
    Iteration 40, loss = 0.03391815
    Iteration 41, loss = 0.03371435
    Iteration 42, loss = 0.03226408
    Iteration 43, loss = 0.03081211
    Iteration 44, loss = 0.02864474
    Iteration 45, loss = 0.02736147
    Iteration 46, loss = 0.02705087
    Iteration 47, loss = 0.02662606
    Iteration 48, loss = 0.02412417
    Iteration 49, loss = 0.02361490
    Iteration 50, loss = 0.02400010
    Iteration 51, loss = 0.02337940
    Iteration 52, loss = 0.02327979
    Iteration 53, loss = 0.02222424
    Iteration 54, loss = 0.02056758
    Iteration 55, loss = 0.02243755
    Iteration 56, loss = 0.02071028
    Iteration 57, loss = 0.02199512
    Iteration 58, loss = 0.01987681
    Iteration 59, loss = 0.01828239
    Iteration 60, loss = 0.01775927
    Iteration 61, loss = 0.01787847
    Iteration 62, loss = 0.01986898
    Iteration 63, loss = 0.01599835
    Iteration 64, loss = 0.01483982
    Iteration 65, loss = 0.01485956
    Iteration 66, loss = 0.01397810
    Iteration 67, loss = 0.01385834
    Iteration 68, loss = 0.01365981
    Iteration 69, loss = 0.01300525
    Iteration 70, loss = 0.01161744
    Iteration 71, loss = 0.01218320
    Iteration 72, loss = 0.01097338
    Iteration 73, loss = 0.01159686
    Iteration 74, loss = 0.01009851
    Iteration 75, loss = 0.01021311
    Iteration 76, loss = 0.00986425
    Iteration 77, loss = 0.00972257
    Iteration 78, loss = 0.00972915
    Iteration 79, loss = 0.00918419
    Iteration 80, loss = 0.00964135
    Iteration 81, loss = 0.00918248
    Iteration 82, loss = 0.00857702
    Iteration 83, loss = 0.00775507
    Iteration 84, loss = 0.00846607
    Iteration 85, loss = 0.00733975
    Iteration 86, loss = 0.00713412
    Iteration 87, loss = 0.00698591
    Iteration 88, loss = 0.00724549
    Iteration 89, loss = 0.00774476
    Iteration 90, loss = 0.00788303
    Iteration 91, loss = 0.00674298
    Iteration 92, loss = 0.00649427
    Iteration 93, loss = 0.00571002
    Iteration 94, loss = 0.00571961
    Iteration 95, loss = 0.00571567
    Iteration 96, loss = 0.00620693
    Iteration 97, loss = 0.00563852
    Iteration 98, loss = 0.00515560
    Iteration 99, loss = 0.00524500
    Iteration 100, loss = 0.00534104
    Iteration 101, loss = 0.00481415
    Iteration 102, loss = 0.00522506
    Iteration 103, loss = 0.00542569
    Iteration 104, loss = 0.00479006
    Iteration 105, loss = 0.00516972
    Iteration 106, loss = 0.00529178
    Iteration 107, loss = 0.00496527
    Iteration 108, loss = 0.00450049
    Iteration 109, loss = 0.00434111
    Iteration 110, loss = 0.00415194
    Iteration 111, loss = 0.00372813
    Iteration 112, loss = 0.00381541
    Iteration 113, loss = 0.00363897
    Iteration 114, loss = 0.00336516
    Iteration 115, loss = 0.00339120
    Iteration 116, loss = 0.00326698
    Iteration 117, loss = 0.00318191
    Iteration 118, loss = 0.00325732
    Iteration 119, loss = 0.00305214
    Iteration 120, loss = 0.00306680
    Iteration 121, loss = 0.00307030
    Iteration 122, loss = 0.00307312
    Iteration 123, loss = 0.00301252
    Iteration 124, loss = 0.00256605
    Iteration 125, loss = 0.00257479
    Iteration 126, loss = 0.00267370
    Iteration 127, loss = 0.00246315
    Iteration 128, loss = 0.00259736
    Iteration 129, loss = 0.00239464
    Iteration 130, loss = 0.00232039
    Iteration 131, loss = 0.00259687
    Iteration 132, loss = 0.00213739
    Iteration 133, loss = 0.00235567
    Iteration 134, loss = 0.00241197
    Iteration 135, loss = 0.00237341
    Iteration 136, loss = 0.00234416
    Iteration 137, loss = 0.00234742
    Iteration 138, loss = 0.00223876
    Iteration 139, loss = 0.00212225
    Iteration 140, loss = 0.00201816
    Iteration 141, loss = 0.00220911
    Iteration 142, loss = 0.00203373
    Iteration 143, loss = 0.00196814
    Iteration 144, loss = 0.00209180
    Iteration 145, loss = 0.00174595
    Iteration 146, loss = 0.00191819
    Iteration 147, loss = 0.00184917
    Iteration 148, loss = 0.00189650
    Iteration 149, loss = 0.00239097
    Iteration 150, loss = 0.00223491
    Iteration 151, loss = 0.00182942
    Iteration 152, loss = 0.00174445
    Iteration 153, loss = 0.00172387
    Iteration 154, loss = 0.00159180
    Iteration 155, loss = 0.00189323
    Iteration 156, loss = 0.00157380
    Iteration 157, loss = 0.00162217
    Iteration 158, loss = 0.00150164
    Iteration 159, loss = 0.00145942
    Iteration 160, loss = 0.00186559
    Iteration 161, loss = 0.00212282
    Iteration 162, loss = 0.00167097
    Iteration 163, loss = 0.00181830
    Iteration 164, loss = 0.00142211
    Iteration 165, loss = 0.00156314
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 9.95e+00 seconds
    Evaluate trained model with onnx: 1.10e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 99-100

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 100-111

.. code-block:: Python

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_onnx, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {num_train} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Apparent Magnitude")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()



.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002_2_00x.png 2.00x
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 41.940 seconds)


.. _sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: neural_network_brightness.ipynb <neural_network_brightness.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: neural_network_brightness.py <neural_network_brightness.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
