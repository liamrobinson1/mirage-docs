
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/01-light_curves/neural_network_brightness.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_01-light_curves_neural_network_brightness.py:


Neural Network Brightness
=========================

Trains a neural network to predict the brightness of a specular cube in an arbitrary lighting and observation conditions and compares the results to the truth

.. GENERATED FROM PYTHON SOURCE LINES 7-15

.. code-block:: Python


    import matplotlib.pyplot as plt
    import numpy as np
    import seaborn as sns

    import mirage as mr
    import mirage.sim as mrsim








.. GENERATED FROM PYTHON SOURCE LINES 16-17

Let's define the object and the BRDF

.. GENERATED FROM PYTHON SOURCE LINES 17-19

.. code-block:: Python

    obj = mr.SpaceObject("cube.obj")
    brdf = mr.Brdf("phong", cd=0.5, cs=0.5, n=10)







.. GENERATED FROM PYTHON SOURCE LINES 20-21

We now define the Multi-Layer Perceptron (MLP) brightness model. Note that the ``layers=(150, 50, 150)`` keyword argument defines the number of neurons in each densely-connected layer.

.. GENERATED FROM PYTHON SOURCE LINES 21-22

.. code-block:: Python

    mlp_bm = mrsim.MLPBrightnessModel(obj, brdf, use_engine=False)







.. GENERATED FROM PYTHON SOURCE LINES 23-24

Now we train the model on a set number of training lighting and observation configurations. Usually ``1e5``-``1e6`` are required for a *good* fit

.. GENERATED FROM PYTHON SOURCE LINES 24-27

.. code-block:: Python

    num_train = int(1e3)
    mlp_bm.train(num_train)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 1.48e-03 seconds
    Iteration 1, loss = 0.27554784
    Iteration 2, loss = 0.16851472
    Iteration 3, loss = 0.15764039
    Iteration 4, loss = 0.14546729
    Iteration 5, loss = 0.14697766
    Iteration 6, loss = 0.14280234
    Iteration 7, loss = 0.14347211
    Iteration 8, loss = 0.14145381
    Iteration 9, loss = 0.14072900
    Iteration 10, loss = 0.13891972
    Iteration 11, loss = 0.13783546
    Iteration 12, loss = 0.13679532
    Iteration 13, loss = 0.13546926
    Iteration 14, loss = 0.13444155
    Iteration 15, loss = 0.13307614
    Iteration 16, loss = 0.13161213
    Iteration 17, loss = 0.13002976
    Iteration 18, loss = 0.12795331
    Iteration 19, loss = 0.12634245
    Iteration 20, loss = 0.12390492
    Iteration 21, loss = 0.12051848
    Iteration 22, loss = 0.11956352
    Iteration 23, loss = 0.11666208
    Iteration 24, loss = 0.11301147
    Iteration 25, loss = 0.11094300
    Iteration 26, loss = 0.10586260
    Iteration 27, loss = 0.10184364
    Iteration 28, loss = 0.09880385
    Iteration 29, loss = 0.09519315
    Iteration 30, loss = 0.09087361
    Iteration 31, loss = 0.08812310
    Iteration 32, loss = 0.08322481
    Iteration 33, loss = 0.07998904
    Iteration 34, loss = 0.07688313
    Iteration 35, loss = 0.07375764
    Iteration 36, loss = 0.06907783
    Iteration 37, loss = 0.06643648
    Iteration 38, loss = 0.06786527
    Iteration 39, loss = 0.06337206
    Iteration 40, loss = 0.05794630
    Iteration 41, loss = 0.05412562
    Iteration 42, loss = 0.05085554
    Iteration 43, loss = 0.04843262
    Iteration 44, loss = 0.04603314
    Iteration 45, loss = 0.04693836
    Iteration 46, loss = 0.04675589
    Iteration 47, loss = 0.04228413
    Iteration 48, loss = 0.03765632
    Iteration 49, loss = 0.03713813
    Iteration 50, loss = 0.03547000
    Iteration 51, loss = 0.03481805
    Iteration 52, loss = 0.03401555
    Iteration 53, loss = 0.03234085
    Iteration 54, loss = 0.03201695
    Iteration 55, loss = 0.03168790
    Iteration 56, loss = 0.02853005
    Iteration 57, loss = 0.02551730
    Iteration 58, loss = 0.02512661
    Iteration 59, loss = 0.02554584
    Iteration 60, loss = 0.02350974
    Iteration 61, loss = 0.02093080
    Iteration 62, loss = 0.02020579
    Iteration 63, loss = 0.01977486
    Iteration 64, loss = 0.01911453
    Iteration 65, loss = 0.01736102
    Iteration 66, loss = 0.01802716
    Iteration 67, loss = 0.01710221
    Iteration 68, loss = 0.01688516
    Iteration 69, loss = 0.01685157
    Iteration 70, loss = 0.01602183
    Iteration 71, loss = 0.01572838
    Iteration 72, loss = 0.01407297
    Iteration 73, loss = 0.01372502
    Iteration 74, loss = 0.01396514
    Iteration 75, loss = 0.01290485
    Iteration 76, loss = 0.01196142
    Iteration 77, loss = 0.01173362
    Iteration 78, loss = 0.01199041
    Iteration 79, loss = 0.01308286
    Iteration 80, loss = 0.01190274
    Iteration 81, loss = 0.01124000
    Iteration 82, loss = 0.01056773
    Iteration 83, loss = 0.00983606
    Iteration 84, loss = 0.00945850
    Iteration 85, loss = 0.00900290
    Iteration 86, loss = 0.00869327
    Iteration 87, loss = 0.00832977
    Iteration 88, loss = 0.00853399
    Iteration 89, loss = 0.00801496
    Iteration 90, loss = 0.00788980
    Iteration 91, loss = 0.00756930
    Iteration 92, loss = 0.00704556
    Iteration 93, loss = 0.00702224
    Iteration 94, loss = 0.00676931
    Iteration 95, loss = 0.00630191
    Iteration 96, loss = 0.00609484
    Iteration 97, loss = 0.00593303
    Iteration 98, loss = 0.00555136
    Iteration 99, loss = 0.00563925
    Iteration 100, loss = 0.00551105
    Iteration 101, loss = 0.00566918
    Iteration 102, loss = 0.00553374
    Iteration 103, loss = 0.00534897
    Iteration 104, loss = 0.00495836
    Iteration 105, loss = 0.00484267
    Iteration 106, loss = 0.00463825
    Iteration 107, loss = 0.00468510
    Iteration 108, loss = 0.00500516
    Iteration 109, loss = 0.00500950
    Iteration 110, loss = 0.00473257
    Iteration 111, loss = 0.00484080
    Iteration 112, loss = 0.00439569
    Iteration 113, loss = 0.00449280
    Iteration 114, loss = 0.00391749
    Iteration 115, loss = 0.00381096
    Iteration 116, loss = 0.00383278
    Iteration 117, loss = 0.00390959
    Iteration 118, loss = 0.00363112
    Iteration 119, loss = 0.00383981
    Iteration 120, loss = 0.00390377
    Iteration 121, loss = 0.00371186
    Iteration 122, loss = 0.00364779
    Iteration 123, loss = 0.00322438
    Iteration 124, loss = 0.00347218
    Iteration 125, loss = 0.00326718
    Iteration 126, loss = 0.00328051
    Iteration 127, loss = 0.00314734
    Iteration 128, loss = 0.00323148
    Iteration 129, loss = 0.00309123
    Iteration 130, loss = 0.00292371
    Iteration 131, loss = 0.00284050
    Iteration 132, loss = 0.00275345
    Iteration 133, loss = 0.00281652
    Iteration 134, loss = 0.00262525
    Iteration 135, loss = 0.00261164
    Iteration 136, loss = 0.00269196
    Iteration 137, loss = 0.00287190
    Iteration 138, loss = 0.00288129
    Iteration 139, loss = 0.00278863
    Iteration 140, loss = 0.00282758
    Iteration 141, loss = 0.00319764
    Iteration 142, loss = 0.00271367
    Iteration 143, loss = 0.00237786
    Iteration 144, loss = 0.00232981
    Iteration 145, loss = 0.00244786
    Iteration 146, loss = 0.00230672
    Iteration 147, loss = 0.00235378
    Iteration 148, loss = 0.00213957
    Iteration 149, loss = 0.00197380
    Iteration 150, loss = 0.00185752
    Iteration 151, loss = 0.00186877
    Iteration 152, loss = 0.00164405
    Iteration 153, loss = 0.00170843
    Iteration 154, loss = 0.00171074
    Iteration 155, loss = 0.00177807
    Iteration 156, loss = 0.00188257
    Iteration 157, loss = 0.00184858
    Iteration 158, loss = 0.00174170
    Iteration 159, loss = 0.00163938
    Iteration 160, loss = 0.00149761
    Iteration 161, loss = 0.00153943
    Iteration 162, loss = 0.00155392
    Iteration 163, loss = 0.00164572
    Iteration 164, loss = 0.00175730
    Iteration 165, loss = 0.00141111
    Iteration 166, loss = 0.00148447
    Iteration 167, loss = 0.00145257
    Iteration 168, loss = 0.00152940
    Iteration 169, loss = 0.00143330
    Iteration 170, loss = 0.00141495
    Iteration 171, loss = 0.00140590
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 1.21e+01 seconds




.. GENERATED FROM PYTHON SOURCE LINES 28-29

We can now simulate a torque-free attitude profile to inspect the quality of the fit

.. GENERATED FROM PYTHON SOURCE LINES 29-40

.. code-block:: Python

    t_eval = np.linspace(0, 10, 1000)
    q, _ = mr.propagate_attitude_torque_free(
        np.array([0.0, 0.0, 0.0, 1.0]),
        np.array([1.0, 1.0, 1.0]),
        np.diag([1, 2, 3]),
        t_eval,
    )
    dcm = mr.quat_to_dcm(q)
    ovb = mr.stack_mat_mult_vec(dcm, np.array([[1, 0, 0]]))
    svb = mr.stack_mat_mult_vec(dcm, np.array([[0, 1, 0]]))








.. GENERATED FROM PYTHON SOURCE LINES 41-42

Evaluating the model in its two available formats - as a native ``scikit-learn`` model and as an Open Neural Network eXchange (ONNX) model

.. GENERATED FROM PYTHON SOURCE LINES 42-49

.. code-block:: Python

    mr.tic("Evaluate trained model with sklearn")
    mdl_b_sklearn = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    mr.toc()
    mr.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate trained model with sklearn: 2.22e-03 seconds
    Evaluate trained model with onnx: 1.28e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 50-51

We can save both of these representations to file:

.. GENERATED FROM PYTHON SOURCE LINES 51-54

.. code-block:: Python

    mlp_bm.save_to_file(save_as_format="onnx")
    mlp_bm.save_to_file(save_as_format="sklearn")








.. GENERATED FROM PYTHON SOURCE LINES 55-56

Now we load the model from its ``.onxx`` file we just saved and evaluate the brightness

.. GENERATED FROM PYTHON SOURCE LINES 56-61

.. code-block:: Python

    mlp_bm.load_from_file(mlp_bm.onnx_file_name)
    mr.tic("Evaluate loaded model with onxx")
    mdl_onnx_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with onxx: 1.45e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 62-63

And we do the same for the ``scikit-learn`` ``.plk`` file we saved

.. GENERATED FROM PYTHON SOURCE LINES 63-68

.. code-block:: Python

    mlp_bm.load_from_file(mlp_bm.sklearn_file_name)
    mr.tic("Evaluate loaded model with sklearn")
    mdl_sklearn_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with sklearn: 1.12e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 69-70

We can easily confirm that all four model evaluations have produced the same prediction

.. GENERATED FROM PYTHON SOURCE LINES 70-75

.. code-block:: Python

    print(np.max(np.abs(mdl_b_sklearn - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_sklearn - mdl_sklearn_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_sklearn_loaded)))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    8.6300987334198e-07
    0.0
    0.0
    8.6300987334198e-07




.. GENERATED FROM PYTHON SOURCE LINES 76-77

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 77-89

.. code-block:: Python

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_sklearn, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {num_train} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Normalized brightness")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()




.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001_2_00x.png 2.00x
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 90-91

We can also train on magnitude data instead of irradiance:

.. GENERATED FROM PYTHON SOURCE LINES 91-98

.. code-block:: Python

    mlp_bm = mrsim.MLPBrightnessModel(obj, brdf, use_engine=True)
    mlp_bm.train(num_train)

    mr.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb)
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 6.73e-01 seconds
    Iteration 1, loss = 0.17220134
    Iteration 2, loss = 0.13719400
    Iteration 3, loss = 0.12258951
    Iteration 4, loss = 0.12151565
    Iteration 5, loss = 0.11862037
    Iteration 6, loss = 0.11785370
    Iteration 7, loss = 0.11602230
    Iteration 8, loss = 0.11507856
    Iteration 9, loss = 0.11376673
    Iteration 10, loss = 0.11124223
    Iteration 11, loss = 0.11056008
    Iteration 12, loss = 0.10859550
    Iteration 13, loss = 0.10642868
    Iteration 14, loss = 0.10499457
    Iteration 15, loss = 0.10242892
    Iteration 16, loss = 0.10350922
    Iteration 17, loss = 0.09782993
    Iteration 18, loss = 0.09545022
    Iteration 19, loss = 0.09323945
    Iteration 20, loss = 0.09015535
    Iteration 21, loss = 0.08690137
    Iteration 22, loss = 0.08294055
    Iteration 23, loss = 0.07889249
    Iteration 24, loss = 0.07557812
    Iteration 25, loss = 0.07096182
    Iteration 26, loss = 0.06755359
    Iteration 27, loss = 0.06349739
    Iteration 28, loss = 0.06042888
    Iteration 29, loss = 0.05620927
    Iteration 30, loss = 0.05310252
    Iteration 31, loss = 0.05582711
    Iteration 32, loss = 0.05176588
    Iteration 33, loss = 0.04805309
    Iteration 34, loss = 0.04311444
    Iteration 35, loss = 0.04146963
    Iteration 36, loss = 0.04142527
    Iteration 37, loss = 0.03814720
    Iteration 38, loss = 0.03721410
    Iteration 39, loss = 0.03489304
    Iteration 40, loss = 0.03430230
    Iteration 41, loss = 0.03335846
    Iteration 42, loss = 0.03077238
    Iteration 43, loss = 0.02938889
    Iteration 44, loss = 0.02841798
    Iteration 45, loss = 0.02735028
    Iteration 46, loss = 0.02603249
    Iteration 47, loss = 0.02461756
    Iteration 48, loss = 0.02445794
    Iteration 49, loss = 0.02368650
    Iteration 50, loss = 0.02317674
    Iteration 51, loss = 0.02213531
    Iteration 52, loss = 0.02143827
    Iteration 53, loss = 0.02079431
    Iteration 54, loss = 0.02057256
    Iteration 55, loss = 0.02045510
    Iteration 56, loss = 0.01910677
    Iteration 57, loss = 0.01703136
    Iteration 58, loss = 0.01695726
    Iteration 59, loss = 0.01683755
    Iteration 60, loss = 0.01672378
    Iteration 61, loss = 0.01706543
    Iteration 62, loss = 0.01563802
    Iteration 63, loss = 0.01594377
    Iteration 64, loss = 0.01463271
    Iteration 65, loss = 0.01359488
    Iteration 66, loss = 0.01352530
    Iteration 67, loss = 0.01305307
    Iteration 68, loss = 0.01542215
    Iteration 69, loss = 0.01438838
    Iteration 70, loss = 0.01488700
    Iteration 71, loss = 0.01310968
    Iteration 72, loss = 0.01365151
    Iteration 73, loss = 0.01382404
    Iteration 74, loss = 0.01221125
    Iteration 75, loss = 0.01131958
    Iteration 76, loss = 0.01152435
    Iteration 77, loss = 0.01054312
    Iteration 78, loss = 0.01100360
    Iteration 79, loss = 0.01100029
    Iteration 80, loss = 0.01019831
    Iteration 81, loss = 0.00998627
    Iteration 82, loss = 0.00909207
    Iteration 83, loss = 0.00851570
    Iteration 84, loss = 0.00845551
    Iteration 85, loss = 0.00808832
    Iteration 86, loss = 0.00752991
    Iteration 87, loss = 0.00759316
    Iteration 88, loss = 0.00742809
    Iteration 89, loss = 0.00720689
    Iteration 90, loss = 0.00722651
    Iteration 91, loss = 0.00693169
    Iteration 92, loss = 0.00741717
    Iteration 93, loss = 0.00709641
    Iteration 94, loss = 0.00631290
    Iteration 95, loss = 0.00652907
    Iteration 96, loss = 0.00610864
    Iteration 97, loss = 0.00591043
    Iteration 98, loss = 0.00603307
    Iteration 99, loss = 0.00591978
    Iteration 100, loss = 0.00587506
    Iteration 101, loss = 0.00571485
    Iteration 102, loss = 0.00533731
    Iteration 103, loss = 0.00535216
    Iteration 104, loss = 0.00520382
    Iteration 105, loss = 0.00510430
    Iteration 106, loss = 0.00513147
    Iteration 107, loss = 0.00515169
    Iteration 108, loss = 0.00454319
    Iteration 109, loss = 0.00484467
    Iteration 110, loss = 0.00424136
    Iteration 111, loss = 0.00474971
    Iteration 112, loss = 0.00446270
    Iteration 113, loss = 0.00399066
    Iteration 114, loss = 0.00469953
    Iteration 115, loss = 0.00422441
    Iteration 116, loss = 0.00443700
    Iteration 117, loss = 0.00514054
    Iteration 118, loss = 0.00465295
    Iteration 119, loss = 0.00398469
    Iteration 120, loss = 0.00378293
    Iteration 121, loss = 0.00326957
    Iteration 122, loss = 0.00317427
    Iteration 123, loss = 0.00322281
    Iteration 124, loss = 0.00350090
    Iteration 125, loss = 0.00357611
    Iteration 126, loss = 0.00360564
    Iteration 127, loss = 0.00372232
    Iteration 128, loss = 0.00325634
    Iteration 129, loss = 0.00325153
    Iteration 130, loss = 0.00282312
    Iteration 131, loss = 0.00274369
    Iteration 132, loss = 0.00275750
    Iteration 133, loss = 0.00261788
    Iteration 134, loss = 0.00302564
    Iteration 135, loss = 0.00262503
    Iteration 136, loss = 0.00265835
    Iteration 137, loss = 0.00289645
    Iteration 138, loss = 0.00290521
    Iteration 139, loss = 0.00247503
    Iteration 140, loss = 0.00260071
    Iteration 141, loss = 0.00248129
    Iteration 142, loss = 0.00223654
    Iteration 143, loss = 0.00221364
    Iteration 144, loss = 0.00250246
    Iteration 145, loss = 0.00262126
    Iteration 146, loss = 0.00249566
    Iteration 147, loss = 0.00198974
    Iteration 148, loss = 0.00200408
    Iteration 149, loss = 0.00194693
    Iteration 150, loss = 0.00175901
    Iteration 151, loss = 0.00185995
    Iteration 152, loss = 0.00191569
    Iteration 153, loss = 0.00186984
    Iteration 154, loss = 0.00202160
    Iteration 155, loss = 0.00164270
    Iteration 156, loss = 0.00176359
    Iteration 157, loss = 0.00150462
    Iteration 158, loss = 0.00142265
    Iteration 159, loss = 0.00137467
    Iteration 160, loss = 0.00134447
    Iteration 161, loss = 0.00134530
    Iteration 162, loss = 0.00132048
    Iteration 163, loss = 0.00144674
    Iteration 164, loss = 0.00146593
    Iteration 165, loss = 0.00139957
    Iteration 166, loss = 0.00127647
    Iteration 167, loss = 0.00129193
    Iteration 168, loss = 0.00136633
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 3.70e+00 seconds
    Evaluate trained model with onnx: 1.18e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 99-100

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 100-111

.. code-block:: Python

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_onnx, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {num_train} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Apparent Magnitude")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()



.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002_2_00x.png 2.00x
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 17.934 seconds)


.. _sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: neural_network_brightness.ipynb <neural_network_brightness.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: neural_network_brightness.py <neural_network_brightness.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
