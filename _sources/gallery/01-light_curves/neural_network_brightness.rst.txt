
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/01-light_curves/neural_network_brightness.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_01-light_curves_neural_network_brightness.py:


Neural Network Brightness
=========================

Trains a neural network to predict the brightness of a specular cube in an arbitrary lighting and observation conditions and compares the results to the truth

.. GENERATED FROM PYTHON SOURCE LINES 7-15

.. code-block:: default


    import matplotlib.pyplot as plt
    import numpy as np
    import seaborn as sns

    import mirage as mr
    import mirage.sim as mrsim








.. GENERATED FROM PYTHON SOURCE LINES 16-17

Let's define the object and the BRDF

.. GENERATED FROM PYTHON SOURCE LINES 17-19

.. code-block:: default

    obj = mr.SpaceObject("cube.obj")
    brdf = mr.Brdf("phong", cd=0.5, cs=0.5, n=10)







.. GENERATED FROM PYTHON SOURCE LINES 20-21

We now define the Multi-Layer Perceptron (MLP) brightness model. Note that the ``layers=(150, 50, 150)`` keyword argument defines the number of neurons in each densely-connected layer.

.. GENERATED FROM PYTHON SOURCE LINES 21-22

.. code-block:: default

    mlp_bm = mrsim.MLPBrightnessModel(obj, brdf, use_engine=False)







.. GENERATED FROM PYTHON SOURCE LINES 23-24

Now we train the model on a set number of training lighting and observation configurations. Usually ``1e5``-``1e6`` are required for a *good* fit

.. GENERATED FROM PYTHON SOURCE LINES 24-27

.. code-block:: default

    num_train = int(1e3)
    mlp_bm.train(num_train)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 1.37e-03 seconds
    Iteration 1, loss = 0.18640655
    Iteration 2, loss = 0.15575577
    Iteration 3, loss = 0.14728158
    Iteration 4, loss = 0.14508654
    Iteration 5, loss = 0.14229904
    Iteration 6, loss = 0.13996128
    Iteration 7, loss = 0.13767409
    Iteration 8, loss = 0.13640737
    Iteration 9, loss = 0.13599407
    Iteration 10, loss = 0.13229363
    Iteration 11, loss = 0.13377047
    Iteration 12, loss = 0.13053858
    Iteration 13, loss = 0.12732544
    Iteration 14, loss = 0.12573576
    Iteration 15, loss = 0.12334431
    Iteration 16, loss = 0.12077147
    Iteration 17, loss = 0.11786989
    Iteration 18, loss = 0.11463537
    Iteration 19, loss = 0.11153095
    Iteration 20, loss = 0.10823267
    Iteration 21, loss = 0.10744572
    Iteration 22, loss = 0.10344025
    Iteration 23, loss = 0.10263654
    Iteration 24, loss = 0.09271334
    Iteration 25, loss = 0.08854728
    Iteration 26, loss = 0.08247638
    Iteration 27, loss = 0.07890122
    Iteration 28, loss = 0.07624886
    Iteration 29, loss = 0.07270751
    Iteration 30, loss = 0.06791523
    Iteration 31, loss = 0.06764977
    Iteration 32, loss = 0.06060192
    Iteration 33, loss = 0.06149766
    Iteration 34, loss = 0.05981814
    Iteration 35, loss = 0.05488924
    Iteration 36, loss = 0.05296856
    Iteration 37, loss = 0.05029834
    Iteration 38, loss = 0.04917196
    Iteration 39, loss = 0.04660459
    Iteration 40, loss = 0.04404874
    Iteration 41, loss = 0.04504831
    Iteration 42, loss = 0.04598490
    Iteration 43, loss = 0.04116020
    Iteration 44, loss = 0.04016838
    Iteration 45, loss = 0.03992797
    Iteration 46, loss = 0.03892360
    Iteration 47, loss = 0.03517198
    Iteration 48, loss = 0.03388539
    Iteration 49, loss = 0.03554664
    Iteration 50, loss = 0.03190827
    Iteration 51, loss = 0.03197058
    Iteration 52, loss = 0.03062019
    Iteration 53, loss = 0.02950079
    Iteration 54, loss = 0.02947205
    Iteration 55, loss = 0.02803599
    Iteration 56, loss = 0.02878961
    Iteration 57, loss = 0.02493639
    Iteration 58, loss = 0.02464131
    Iteration 59, loss = 0.02412093
    Iteration 60, loss = 0.02449063
    Iteration 61, loss = 0.02278626
    Iteration 62, loss = 0.02166814
    Iteration 63, loss = 0.02074595
    Iteration 64, loss = 0.02014040
    Iteration 65, loss = 0.02081340
    Iteration 66, loss = 0.02008241
    Iteration 67, loss = 0.02013220
    Iteration 68, loss = 0.01798964
    Iteration 69, loss = 0.01755624
    Iteration 70, loss = 0.01682131
    Iteration 71, loss = 0.01597040
    Iteration 72, loss = 0.01554744
    Iteration 73, loss = 0.01500049
    Iteration 74, loss = 0.01414193
    Iteration 75, loss = 0.01444373
    Iteration 76, loss = 0.01341064
    Iteration 77, loss = 0.01355059
    Iteration 78, loss = 0.01336937
    Iteration 79, loss = 0.01493382
    Iteration 80, loss = 0.01410720
    Iteration 81, loss = 0.01323098
    Iteration 82, loss = 0.01345563
    Iteration 83, loss = 0.01279766
    Iteration 84, loss = 0.01141958
    Iteration 85, loss = 0.01183360
    Iteration 86, loss = 0.01136439
    Iteration 87, loss = 0.01096362
    Iteration 88, loss = 0.01005363
    Iteration 89, loss = 0.01019649
    Iteration 90, loss = 0.00965656
    Iteration 91, loss = 0.00912932
    Iteration 92, loss = 0.00967448
    Iteration 93, loss = 0.00926196
    Iteration 94, loss = 0.00922133
    Iteration 95, loss = 0.00961330
    Iteration 96, loss = 0.00936107
    Iteration 97, loss = 0.00839520
    Iteration 98, loss = 0.00768405
    Iteration 99, loss = 0.00736835
    Iteration 100, loss = 0.00721389
    Iteration 101, loss = 0.00694482
    Iteration 102, loss = 0.00690465
    Iteration 103, loss = 0.00708035
    Iteration 104, loss = 0.00719384
    Iteration 105, loss = 0.00730698
    Iteration 106, loss = 0.00817959
    Iteration 107, loss = 0.00723644
    Iteration 108, loss = 0.00668552
    Iteration 109, loss = 0.00637093
    Iteration 110, loss = 0.00587414
    Iteration 111, loss = 0.00586843
    Iteration 112, loss = 0.00577995
    Iteration 113, loss = 0.00567185
    Iteration 114, loss = 0.00558643
    Iteration 115, loss = 0.00558274
    Iteration 116, loss = 0.00496888
    Iteration 117, loss = 0.00532907
    Iteration 118, loss = 0.00559158
    Iteration 119, loss = 0.00531540
    Iteration 120, loss = 0.00508847
    Iteration 121, loss = 0.00488871
    Iteration 122, loss = 0.00493396
    Iteration 123, loss = 0.00480497
    Iteration 124, loss = 0.00468480
    Iteration 125, loss = 0.00439818
    Iteration 126, loss = 0.00448908
    Iteration 127, loss = 0.00435423
    Iteration 128, loss = 0.00407961
    Iteration 129, loss = 0.00405324
    Iteration 130, loss = 0.00621019
    Iteration 131, loss = 0.00625657
    Iteration 132, loss = 0.00505033
    Iteration 133, loss = 0.00491467
    Iteration 134, loss = 0.00420467
    Iteration 135, loss = 0.00412883
    Iteration 136, loss = 0.00380567
    Iteration 137, loss = 0.00359281
    Iteration 138, loss = 0.00380290
    Iteration 139, loss = 0.00394204
    Iteration 140, loss = 0.00543475
    Iteration 141, loss = 0.00403524
    Iteration 142, loss = 0.00381055
    Iteration 143, loss = 0.00326771
    Iteration 144, loss = 0.00330068
    Iteration 145, loss = 0.00358073
    Iteration 146, loss = 0.00332058
    Iteration 147, loss = 0.00326381
    Iteration 148, loss = 0.00310700
    Iteration 149, loss = 0.00302087
    Iteration 150, loss = 0.00296075
    Iteration 151, loss = 0.00253613
    Iteration 152, loss = 0.00247362
    Iteration 153, loss = 0.00272070
    Iteration 154, loss = 0.00277877
    Iteration 155, loss = 0.00270899
    Iteration 156, loss = 0.00252933
    Iteration 157, loss = 0.00240024
    Iteration 158, loss = 0.00250778
    Iteration 159, loss = 0.00265480
    Iteration 160, loss = 0.00286337
    Iteration 161, loss = 0.00265565
    Iteration 162, loss = 0.00273813
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 1.76e+01 seconds




.. GENERATED FROM PYTHON SOURCE LINES 28-29

We can now simulate a torque-free attitude profile to inspect the quality of the fit

.. GENERATED FROM PYTHON SOURCE LINES 29-40

.. code-block:: default

    t_eval = np.linspace(0, 10, 1000)
    q, _ = mr.propagate_attitude_torque_free(
        np.array([0.0, 0.0, 0.0, 1.0]),
        np.array([1.0, 1.0, 1.0]),
        np.diag([1, 2, 3]),
        t_eval,
    )
    dcm = mr.quat_to_dcm(q)
    ovb = mr.stack_mat_mult_vec(dcm, np.array([[1, 0, 0]]))
    svb = mr.stack_mat_mult_vec(dcm, np.array([[0, 1, 0]]))








.. GENERATED FROM PYTHON SOURCE LINES 41-42

Evaluating the model in its two available formats - as a native ``scikit-learn`` model and as an Open Neural Network eXchange (ONNX) model

.. GENERATED FROM PYTHON SOURCE LINES 42-49

.. code-block:: default

    mr.tic("Evaluate trained model with sklearn")
    mdl_b_sklearn = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    mr.toc()
    mr.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate trained model with sklearn: 3.49e-03 seconds
    Evaluate trained model with onnx: 2.38e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 50-51

We can save both of these representations to file:

.. GENERATED FROM PYTHON SOURCE LINES 51-54

.. code-block:: default

    mlp_bm.save_to_file(save_as_format="onnx")
    mlp_bm.save_to_file(save_as_format="sklearn")








.. GENERATED FROM PYTHON SOURCE LINES 55-56

Now we load the model from its ``.onxx`` file we just saved and evaluate the brightness

.. GENERATED FROM PYTHON SOURCE LINES 56-61

.. code-block:: default

    mlp_bm.load_from_file(mlp_bm.onnx_file_name)
    mr.tic("Evaluate loaded model with onxx")
    mdl_onnx_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with onxx: 1.32e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 62-63

And we do the same for the ``scikit-learn`` ``.plk`` file we saved

.. GENERATED FROM PYTHON SOURCE LINES 63-68

.. code-block:: default

    mlp_bm.load_from_file(mlp_bm.sklearn_file_name)
    mr.tic("Evaluate loaded model with sklearn")
    mdl_sklearn_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with sklearn: 9.36e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 69-70

We can easily confirm that all four model evaluations have produced the same prediction

.. GENERATED FROM PYTHON SOURCE LINES 70-75

.. code-block:: default

    print(np.max(np.abs(mdl_b_sklearn - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_sklearn - mdl_sklearn_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_sklearn_loaded)))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    8.324862024178259e-07
    0.0
    0.0
    8.324862024178259e-07




.. GENERATED FROM PYTHON SOURCE LINES 76-77

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 77-89

.. code-block:: default

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_sklearn, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {num_train} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Normalized brightness")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()




.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001_2_00x.png 2.00x
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 90-91

We can also train on magnitude data instead of irradiance:

.. GENERATED FROM PYTHON SOURCE LINES 91-98

.. code-block:: default

    mlp_bm = mrsim.MLPBrightnessModel(obj, brdf, use_engine=True)
    mlp_bm.train(num_train)

    mr.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb)
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 5.18e-01 seconds
    Iteration 1, loss = 0.31649934
    Iteration 2, loss = 0.15492891
    Iteration 3, loss = 0.15045902
    Iteration 4, loss = 0.12348998
    Iteration 5, loss = 0.12439945
    Iteration 6, loss = 0.12126418
    Iteration 7, loss = 0.11997787
    Iteration 8, loss = 0.11937107
    Iteration 9, loss = 0.11741503
    Iteration 10, loss = 0.11625417
    Iteration 11, loss = 0.11540509
    Iteration 12, loss = 0.11445707
    Iteration 13, loss = 0.11363546
    Iteration 14, loss = 0.11227338
    Iteration 15, loss = 0.11160559
    Iteration 16, loss = 0.11051438
    Iteration 17, loss = 0.10936596
    Iteration 18, loss = 0.10810809
    Iteration 19, loss = 0.10682701
    Iteration 20, loss = 0.10564499
    Iteration 21, loss = 0.10468266
    Iteration 22, loss = 0.10242005
    Iteration 23, loss = 0.10150302
    Iteration 24, loss = 0.09948228
    Iteration 25, loss = 0.09792671
    Iteration 26, loss = 0.09491605
    Iteration 27, loss = 0.09256409
    Iteration 28, loss = 0.09281722
    Iteration 29, loss = 0.08755724
    Iteration 30, loss = 0.08481429
    Iteration 31, loss = 0.08195574
    Iteration 32, loss = 0.07776252
    Iteration 33, loss = 0.07624126
    Iteration 34, loss = 0.07190146
    Iteration 35, loss = 0.06801224
    Iteration 36, loss = 0.06367741
    Iteration 37, loss = 0.06057381
    Iteration 38, loss = 0.05863480
    Iteration 39, loss = 0.05521453
    Iteration 40, loss = 0.05326614
    Iteration 41, loss = 0.04992515
    Iteration 42, loss = 0.04730055
    Iteration 43, loss = 0.04486410
    Iteration 44, loss = 0.04353894
    Iteration 45, loss = 0.04309654
    Iteration 46, loss = 0.03952253
    Iteration 47, loss = 0.03968753
    Iteration 48, loss = 0.03723744
    Iteration 49, loss = 0.03596125
    Iteration 50, loss = 0.03328904
    Iteration 51, loss = 0.03101938
    Iteration 52, loss = 0.02921924
    Iteration 53, loss = 0.02831232
    Iteration 54, loss = 0.02719628
    Iteration 55, loss = 0.02630809
    Iteration 56, loss = 0.02519570
    Iteration 57, loss = 0.02547878
    Iteration 58, loss = 0.02521040
    Iteration 59, loss = 0.02240277
    Iteration 60, loss = 0.02169454
    Iteration 61, loss = 0.02069523
    Iteration 62, loss = 0.01983098
    Iteration 63, loss = 0.01927632
    Iteration 64, loss = 0.01790720
    Iteration 65, loss = 0.01734582
    Iteration 66, loss = 0.01715732
    Iteration 67, loss = 0.01628710
    Iteration 68, loss = 0.01656992
    Iteration 69, loss = 0.01541827
    Iteration 70, loss = 0.01384210
    Iteration 71, loss = 0.01300420
    Iteration 72, loss = 0.01259090
    Iteration 73, loss = 0.01211041
    Iteration 74, loss = 0.01135192
    Iteration 75, loss = 0.01150337
    Iteration 76, loss = 0.01167251
    Iteration 77, loss = 0.01033727
    Iteration 78, loss = 0.01029883
    Iteration 79, loss = 0.01000541
    Iteration 80, loss = 0.01008975
    Iteration 81, loss = 0.00917654
    Iteration 82, loss = 0.00907254
    Iteration 83, loss = 0.00812760
    Iteration 84, loss = 0.00831991
    Iteration 85, loss = 0.00885011
    Iteration 86, loss = 0.00823466
    Iteration 87, loss = 0.00810368
    Iteration 88, loss = 0.00769279
    Iteration 89, loss = 0.00675316
    Iteration 90, loss = 0.00641677
    Iteration 91, loss = 0.00670891
    Iteration 92, loss = 0.00632439
    Iteration 93, loss = 0.00583974
    Iteration 94, loss = 0.00572289
    Iteration 95, loss = 0.00546625
    Iteration 96, loss = 0.00533053
    Iteration 97, loss = 0.00540457
    Iteration 98, loss = 0.00590385
    Iteration 99, loss = 0.00516845
    Iteration 100, loss = 0.00514842
    Iteration 101, loss = 0.00531107
    Iteration 102, loss = 0.00453771
    Iteration 103, loss = 0.00503087
    Iteration 104, loss = 0.00461439
    Iteration 105, loss = 0.00445983
    Iteration 106, loss = 0.00453413
    Iteration 107, loss = 0.00452953
    Iteration 108, loss = 0.00414387
    Iteration 109, loss = 0.00415187
    Iteration 110, loss = 0.00432646
    Iteration 111, loss = 0.00447863
    Iteration 112, loss = 0.00454570
    Iteration 113, loss = 0.00405798
    Iteration 114, loss = 0.00393791
    Iteration 115, loss = 0.00412742
    Iteration 116, loss = 0.00383238
    Iteration 117, loss = 0.00373115
    Iteration 118, loss = 0.00366007
    Iteration 119, loss = 0.00384445
    Iteration 120, loss = 0.00296667
    Iteration 121, loss = 0.00309855
    Iteration 122, loss = 0.00296962
    Iteration 123, loss = 0.00293326
    Iteration 124, loss = 0.00283114
    Iteration 125, loss = 0.00278375
    Iteration 126, loss = 0.00296170
    Iteration 127, loss = 0.00247962
    Iteration 128, loss = 0.00259081
    Iteration 129, loss = 0.00272133
    Iteration 130, loss = 0.00277937
    Iteration 131, loss = 0.00242575
    Iteration 132, loss = 0.00217323
    Iteration 133, loss = 0.00209916
    Iteration 134, loss = 0.00221493
    Iteration 135, loss = 0.00216668
    Iteration 136, loss = 0.00196306
    Iteration 137, loss = 0.00193032
    Iteration 138, loss = 0.00191089
    Iteration 139, loss = 0.00177530
    Iteration 140, loss = 0.00174217
    Iteration 141, loss = 0.00168308
    Iteration 142, loss = 0.00191599
    Iteration 143, loss = 0.00196265
    Iteration 144, loss = 0.00169140
    Iteration 145, loss = 0.00183773
    Iteration 146, loss = 0.00181533
    Iteration 147, loss = 0.00195823
    Iteration 148, loss = 0.00166386
    Iteration 149, loss = 0.00161293
    Iteration 150, loss = 0.00155857
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 4.12e+00 seconds
    Evaluate trained model with onnx: 1.34e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 99-100

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 100-111

.. code-block:: default

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_onnx, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {num_train} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Apparent Magnitude")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()



.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002_2_00x.png 2.00x
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  23.478 seconds)


.. _sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: neural_network_brightness.py <neural_network_brightness.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: neural_network_brightness.ipynb <neural_network_brightness.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
