
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/01-light_curves/neural_network_brightness.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_01-light_curves_neural_network_brightness.py:


Neural Network Brightness
=========================

Trains a neural network to predict the brightness of a specular cube in an arbitrary lighting and observation conditions and compares the results to the truth

.. GENERATED FROM PYTHON SOURCE LINES 7-15

.. code-block:: Python


    import matplotlib.pyplot as plt
    import numpy as np
    import seaborn as sns

    import mirage as mr
    import mirage.sim as mrsim








.. GENERATED FROM PYTHON SOURCE LINES 16-17

Let's define the object and the BRDF

.. GENERATED FROM PYTHON SOURCE LINES 17-19

.. code-block:: Python

    obj = mr.SpaceObject("cube.obj")
    brdf = mr.Brdf("phong", cd=0.5, cs=0.5, n=10)







.. GENERATED FROM PYTHON SOURCE LINES 20-21

We now define the Multi-Layer Perceptron (MLP) brightness model. Note that the ``layers=(150, 50, 150)`` keyword argument defines the number of neurons in each densely-connected layer.

.. GENERATED FROM PYTHON SOURCE LINES 21-22

.. code-block:: Python

    mlp_bm = mrsim.MLPBrightnessModel(obj, brdf, use_engine=False)







.. GENERATED FROM PYTHON SOURCE LINES 23-24

Now we train the model on a set number of training lighting and observation configurations. Usually ``1e5``-``1e6`` are required for a *good* fit

.. GENERATED FROM PYTHON SOURCE LINES 24-27

.. code-block:: Python

    num_train = int(1e3)
    mlp_bm.train(num_train)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 1.58e-03 seconds
    Iteration 1, loss = 0.32113136
    Iteration 2, loss = 0.17159416
    Iteration 3, loss = 0.17111956
    Iteration 4, loss = 0.15027799
    Iteration 5, loss = 0.14797514
    Iteration 6, loss = 0.14423420
    Iteration 7, loss = 0.14178456
    Iteration 8, loss = 0.14001309
    Iteration 9, loss = 0.13937112
    Iteration 10, loss = 0.13891088
    Iteration 11, loss = 0.13669040
    Iteration 12, loss = 0.13904793
    Iteration 13, loss = 0.13579812
    Iteration 14, loss = 0.13539791
    Iteration 15, loss = 0.13152911
    Iteration 16, loss = 0.13081324
    Iteration 17, loss = 0.12902746
    Iteration 18, loss = 0.12669744
    Iteration 19, loss = 0.12591190
    Iteration 20, loss = 0.12338250
    Iteration 21, loss = 0.12066077
    Iteration 22, loss = 0.11689784
    Iteration 23, loss = 0.11419363
    Iteration 24, loss = 0.11102123
    Iteration 25, loss = 0.10676721
    Iteration 26, loss = 0.10342475
    Iteration 27, loss = 0.10006032
    Iteration 28, loss = 0.09592021
    Iteration 29, loss = 0.09335353
    Iteration 30, loss = 0.08776608
    Iteration 31, loss = 0.08169318
    Iteration 32, loss = 0.07698115
    Iteration 33, loss = 0.07317733
    Iteration 34, loss = 0.06797720
    Iteration 35, loss = 0.06336230
    Iteration 36, loss = 0.06027228
    Iteration 37, loss = 0.05749086
    Iteration 38, loss = 0.05789029
    Iteration 39, loss = 0.05692600
    Iteration 40, loss = 0.05158776
    Iteration 41, loss = 0.04890162
    Iteration 42, loss = 0.04655421
    Iteration 43, loss = 0.04234444
    Iteration 44, loss = 0.04046244
    Iteration 45, loss = 0.04060317
    Iteration 46, loss = 0.03670788
    Iteration 47, loss = 0.03503364
    Iteration 48, loss = 0.03234711
    Iteration 49, loss = 0.03296932
    Iteration 50, loss = 0.03436639
    Iteration 51, loss = 0.03096585
    Iteration 52, loss = 0.02933600
    Iteration 53, loss = 0.03010884
    Iteration 54, loss = 0.02784488
    Iteration 55, loss = 0.02487540
    Iteration 56, loss = 0.02222316
    Iteration 57, loss = 0.02068128
    Iteration 58, loss = 0.01973695
    Iteration 59, loss = 0.01872931
    Iteration 60, loss = 0.01784911
    Iteration 61, loss = 0.01773926
    Iteration 62, loss = 0.01763982
    Iteration 63, loss = 0.01640832
    Iteration 64, loss = 0.01682338
    Iteration 65, loss = 0.01637399
    Iteration 66, loss = 0.01407554
    Iteration 67, loss = 0.01384783
    Iteration 68, loss = 0.01286657
    Iteration 69, loss = 0.01270907
    Iteration 70, loss = 0.01174017
    Iteration 71, loss = 0.01120469
    Iteration 72, loss = 0.01051963
    Iteration 73, loss = 0.01087536
    Iteration 74, loss = 0.01032487
    Iteration 75, loss = 0.00995530
    Iteration 76, loss = 0.00979913
    Iteration 77, loss = 0.00990218
    Iteration 78, loss = 0.00982887
    Iteration 79, loss = 0.00887795
    Iteration 80, loss = 0.00820334
    Iteration 81, loss = 0.00760272
    Iteration 82, loss = 0.00704043
    Iteration 83, loss = 0.00676555
    Iteration 84, loss = 0.00663704
    Iteration 85, loss = 0.00693375
    Iteration 86, loss = 0.00658893
    Iteration 87, loss = 0.00613847
    Iteration 88, loss = 0.00581178
    Iteration 89, loss = 0.00597941
    Iteration 90, loss = 0.00558410
    Iteration 91, loss = 0.00557088
    Iteration 92, loss = 0.00526605
    Iteration 93, loss = 0.00510741
    Iteration 94, loss = 0.00515299
    Iteration 95, loss = 0.00502763
    Iteration 96, loss = 0.00511302
    Iteration 97, loss = 0.00468085
    Iteration 98, loss = 0.00456954
    Iteration 99, loss = 0.00472950
    Iteration 100, loss = 0.00499731
    Iteration 101, loss = 0.00468679
    Iteration 102, loss = 0.00405075
    Iteration 103, loss = 0.00387392
    Iteration 104, loss = 0.00375112
    Iteration 105, loss = 0.00361886
    Iteration 106, loss = 0.00347754
    Iteration 107, loss = 0.00369186
    Iteration 108, loss = 0.00341719
    Iteration 109, loss = 0.00342379
    Iteration 110, loss = 0.00332337
    Iteration 111, loss = 0.00339782
    Iteration 112, loss = 0.00352920
    Iteration 113, loss = 0.00332986
    Iteration 114, loss = 0.00322991
    Iteration 115, loss = 0.00299845
    Iteration 116, loss = 0.00282980
    Iteration 117, loss = 0.00274843
    Iteration 118, loss = 0.00256629
    Iteration 119, loss = 0.00252167
    Iteration 120, loss = 0.00250569
    Iteration 121, loss = 0.00236850
    Iteration 122, loss = 0.00226703
    Iteration 123, loss = 0.00225289
    Iteration 124, loss = 0.00239321
    Iteration 125, loss = 0.00226312
    Iteration 126, loss = 0.00238449
    Iteration 127, loss = 0.00230300
    Iteration 128, loss = 0.00214689
    Iteration 129, loss = 0.00214548
    Iteration 130, loss = 0.00211966
    Iteration 131, loss = 0.00204857
    Iteration 132, loss = 0.00204405
    Iteration 133, loss = 0.00226692
    Iteration 134, loss = 0.00205144
    Iteration 135, loss = 0.00217597
    Iteration 136, loss = 0.00202306
    Iteration 137, loss = 0.00204857
    Iteration 138, loss = 0.00173390
    Iteration 139, loss = 0.00164854
    Iteration 140, loss = 0.00167232
    Iteration 141, loss = 0.00164658
    Iteration 142, loss = 0.00156381
    Iteration 143, loss = 0.00170508
    Iteration 144, loss = 0.00161017
    Iteration 145, loss = 0.00161488
    Iteration 146, loss = 0.00180938
    Iteration 147, loss = 0.00185878
    Iteration 148, loss = 0.00181586
    Iteration 149, loss = 0.00223324
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 1.50e+01 seconds




.. GENERATED FROM PYTHON SOURCE LINES 28-29

We can now simulate a torque-free attitude profile to inspect the quality of the fit

.. GENERATED FROM PYTHON SOURCE LINES 29-40

.. code-block:: Python

    t_eval = np.linspace(0, 10, 1000)
    q, _ = mr.propagate_attitude_torque_free(
        np.array([0.0, 0.0, 0.0, 1.0]),
        np.array([1.0, 1.0, 1.0]),
        np.diag([1, 2, 3]),
        t_eval,
    )
    dcm = mr.quat_to_dcm(q)
    ovb = mr.stack_mat_mult_vec(dcm, np.array([[1, 0, 0]]))
    svb = mr.stack_mat_mult_vec(dcm, np.array([[0, 1, 0]]))








.. GENERATED FROM PYTHON SOURCE LINES 41-42

Evaluating the model in its two available formats - as a native ``scikit-learn`` model and as an Open Neural Network eXchange (ONNX) model

.. GENERATED FROM PYTHON SOURCE LINES 42-49

.. code-block:: Python

    mr.tic("Evaluate trained model with sklearn")
    mdl_b_sklearn = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    mr.toc()
    mr.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate trained model with sklearn: 4.07e-03 seconds
    Evaluate trained model with onnx: 1.98e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 50-51

We can save both of these representations to file:

.. GENERATED FROM PYTHON SOURCE LINES 51-54

.. code-block:: Python

    mlp_bm.save_to_file(save_as_format="onnx")
    mlp_bm.save_to_file(save_as_format="sklearn")








.. GENERATED FROM PYTHON SOURCE LINES 55-56

Now we load the model from its ``.onxx`` file we just saved and evaluate the brightness

.. GENERATED FROM PYTHON SOURCE LINES 56-61

.. code-block:: Python

    mlp_bm.load_from_file(mlp_bm.onnx_file_name)
    mr.tic("Evaluate loaded model with onxx")
    mdl_onnx_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with onxx: 1.10e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 62-63

And we do the same for the ``scikit-learn`` ``.plk`` file we saved

.. GENERATED FROM PYTHON SOURCE LINES 63-68

.. code-block:: Python

    mlp_bm.load_from_file(mlp_bm.sklearn_file_name)
    mr.tic("Evaluate loaded model with sklearn")
    mdl_sklearn_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with sklearn: 1.07e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 69-70

We can easily confirm that all four model evaluations have produced the same prediction

.. GENERATED FROM PYTHON SOURCE LINES 70-75

.. code-block:: Python

    print(np.max(np.abs(mdl_b_sklearn - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_sklearn - mdl_sklearn_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_sklearn_loaded)))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    8.377445075424816e-07
    0.0
    0.0
    8.377445075424816e-07




.. GENERATED FROM PYTHON SOURCE LINES 76-77

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 77-89

.. code-block:: Python

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_sklearn, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {num_train} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Normalized brightness")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()




.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001_2_00x.png 2.00x
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 90-91

We can also train on magnitude data instead of irradiance:

.. GENERATED FROM PYTHON SOURCE LINES 91-98

.. code-block:: Python

    mlp_bm = mrsim.MLPBrightnessModel(obj, brdf, use_engine=True)
    mlp_bm.train(num_train)

    mr.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb)
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 6.35e-01 seconds
    Iteration 1, loss = 0.18953072
    Iteration 2, loss = 0.14632995
    Iteration 3, loss = 0.12967292
    Iteration 4, loss = 0.12366151
    Iteration 5, loss = 0.12237631
    Iteration 6, loss = 0.12062881
    Iteration 7, loss = 0.11925443
    Iteration 8, loss = 0.11853234
    Iteration 9, loss = 0.11746625
    Iteration 10, loss = 0.11585963
    Iteration 11, loss = 0.11521448
    Iteration 12, loss = 0.11387418
    Iteration 13, loss = 0.11268944
    Iteration 14, loss = 0.11193483
    Iteration 15, loss = 0.10999915
    Iteration 16, loss = 0.10931027
    Iteration 17, loss = 0.10730359
    Iteration 18, loss = 0.10589748
    Iteration 19, loss = 0.10457602
    Iteration 20, loss = 0.10327126
    Iteration 21, loss = 0.09945256
    Iteration 22, loss = 0.09663980
    Iteration 23, loss = 0.09428814
    Iteration 24, loss = 0.09269386
    Iteration 25, loss = 0.09096200
    Iteration 26, loss = 0.08881865
    Iteration 27, loss = 0.08218992
    Iteration 28, loss = 0.07810266
    Iteration 29, loss = 0.07418550
    Iteration 30, loss = 0.07182551
    Iteration 31, loss = 0.06951079
    Iteration 32, loss = 0.06358650
    Iteration 33, loss = 0.06032774
    Iteration 34, loss = 0.05826661
    Iteration 35, loss = 0.05424586
    Iteration 36, loss = 0.05004214
    Iteration 37, loss = 0.04860504
    Iteration 38, loss = 0.04533800
    Iteration 39, loss = 0.04281995
    Iteration 40, loss = 0.04548825
    Iteration 41, loss = 0.04089711
    Iteration 42, loss = 0.03769627
    Iteration 43, loss = 0.03748354
    Iteration 44, loss = 0.03503920
    Iteration 45, loss = 0.03468870
    Iteration 46, loss = 0.03329027
    Iteration 47, loss = 0.03201340
    Iteration 48, loss = 0.03210840
    Iteration 49, loss = 0.02973416
    Iteration 50, loss = 0.02870447
    Iteration 51, loss = 0.03030125
    Iteration 52, loss = 0.02837184
    Iteration 53, loss = 0.02652756
    Iteration 54, loss = 0.02424290
    Iteration 55, loss = 0.02492061
    Iteration 56, loss = 0.02499893
    Iteration 57, loss = 0.02557986
    Iteration 58, loss = 0.02289401
    Iteration 59, loss = 0.02215881
    Iteration 60, loss = 0.02101587
    Iteration 61, loss = 0.01956430
    Iteration 62, loss = 0.01978719
    Iteration 63, loss = 0.01832189
    Iteration 64, loss = 0.01774740
    Iteration 65, loss = 0.01729663
    Iteration 66, loss = 0.01709492
    Iteration 67, loss = 0.01711827
    Iteration 68, loss = 0.01630427
    Iteration 69, loss = 0.01571985
    Iteration 70, loss = 0.01899195
    Iteration 71, loss = 0.01666338
    Iteration 72, loss = 0.01485610
    Iteration 73, loss = 0.01443401
    Iteration 74, loss = 0.01619188
    Iteration 75, loss = 0.01513425
    Iteration 76, loss = 0.01478613
    Iteration 77, loss = 0.01330648
    Iteration 78, loss = 0.01438710
    Iteration 79, loss = 0.01287396
    Iteration 80, loss = 0.01177334
    Iteration 81, loss = 0.01109059
    Iteration 82, loss = 0.01255447
    Iteration 83, loss = 0.01202878
    Iteration 84, loss = 0.01209726
    Iteration 85, loss = 0.01223698
    Iteration 86, loss = 0.01243474
    Iteration 87, loss = 0.01071614
    Iteration 88, loss = 0.00964959
    Iteration 89, loss = 0.00950742
    Iteration 90, loss = 0.01039531
    Iteration 91, loss = 0.00975364
    Iteration 92, loss = 0.00903808
    Iteration 93, loss = 0.00841892
    Iteration 94, loss = 0.00856903
    Iteration 95, loss = 0.00887915
    Iteration 96, loss = 0.00780325
    Iteration 97, loss = 0.00761020
    Iteration 98, loss = 0.00785993
    Iteration 99, loss = 0.00783683
    Iteration 100, loss = 0.00755035
    Iteration 101, loss = 0.00732655
    Iteration 102, loss = 0.00711399
    Iteration 103, loss = 0.00774698
    Iteration 104, loss = 0.00821284
    Iteration 105, loss = 0.00722727
    Iteration 106, loss = 0.00682910
    Iteration 107, loss = 0.00718075
    Iteration 108, loss = 0.00676630
    Iteration 109, loss = 0.00662561
    Iteration 110, loss = 0.00585603
    Iteration 111, loss = 0.00547146
    Iteration 112, loss = 0.00537153
    Iteration 113, loss = 0.00618966
    Iteration 114, loss = 0.00564067
    Iteration 115, loss = 0.00526505
    Iteration 116, loss = 0.00568463
    Iteration 117, loss = 0.00603956
    Iteration 118, loss = 0.00609505
    Iteration 119, loss = 0.00529161
    Iteration 120, loss = 0.00516255
    Iteration 121, loss = 0.00485112
    Iteration 122, loss = 0.00503586
    Iteration 123, loss = 0.00505151
    Iteration 124, loss = 0.00483014
    Iteration 125, loss = 0.00469195
    Iteration 126, loss = 0.00428576
    Iteration 127, loss = 0.00427542
    Iteration 128, loss = 0.00414405
    Iteration 129, loss = 0.00393517
    Iteration 130, loss = 0.00376103
    Iteration 131, loss = 0.00439045
    Iteration 132, loss = 0.00403095
    Iteration 133, loss = 0.00427782
    Iteration 134, loss = 0.00437482
    Iteration 135, loss = 0.00420100
    Iteration 136, loss = 0.00482794
    Iteration 137, loss = 0.00478169
    Iteration 138, loss = 0.00388390
    Iteration 139, loss = 0.00343157
    Iteration 140, loss = 0.00338265
    Iteration 141, loss = 0.00308951
    Iteration 142, loss = 0.00288276
    Iteration 143, loss = 0.00289027
    Iteration 144, loss = 0.00291213
    Iteration 145, loss = 0.00315124
    Iteration 146, loss = 0.00349295
    Iteration 147, loss = 0.00365242
    Iteration 148, loss = 0.00337176
    Iteration 149, loss = 0.00318295
    Iteration 150, loss = 0.00279723
    Iteration 151, loss = 0.00281800
    Iteration 152, loss = 0.00249628
    Iteration 153, loss = 0.00232247
    Iteration 154, loss = 0.00228205
    Iteration 155, loss = 0.00229749
    Iteration 156, loss = 0.00242023
    Iteration 157, loss = 0.00234054
    Iteration 158, loss = 0.00223714
    Iteration 159, loss = 0.00212797
    Iteration 160, loss = 0.00208569
    Iteration 161, loss = 0.00215568
    Iteration 162, loss = 0.00195548
    Iteration 163, loss = 0.00216754
    Iteration 164, loss = 0.00190247
    Iteration 165, loss = 0.00193557
    Iteration 166, loss = 0.00196592
    Iteration 167, loss = 0.00176407
    Iteration 168, loss = 0.00202746
    Iteration 169, loss = 0.00190845
    Iteration 170, loss = 0.00183235
    Iteration 171, loss = 0.00194247
    Iteration 172, loss = 0.00200132
    Iteration 173, loss = 0.00219951
    Iteration 174, loss = 0.00240130
    Iteration 175, loss = 0.00229620
    Iteration 176, loss = 0.00217958
    Iteration 177, loss = 0.00192909
    Iteration 178, loss = 0.00171933
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 3.42e+00 seconds
    Evaluate trained model with onnx: 1.73e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 99-100

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 100-111

.. code-block:: Python

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_onnx, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {num_train} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Apparent Magnitude")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()



.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002_2_00x.png 2.00x
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 20.578 seconds)


.. _sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: neural_network_brightness.ipynb <neural_network_brightness.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: neural_network_brightness.py <neural_network_brightness.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
