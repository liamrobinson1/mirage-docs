
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/01-light_curves/neural_network_brightness.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_01-light_curves_neural_network_brightness.py:


Neural Network Brightness
=========================

Trains a neural network to predict the brightness of a specular cube in an arbitrary lighting and observation conditions and compares the results to the truth

.. GENERATED FROM PYTHON SOURCE LINES 7-15

.. code-block:: Python


    import matplotlib.pyplot as plt
    import numpy as np
    import seaborn as sns

    import mirage as mr
    import mirage.sim as mrsim








.. GENERATED FROM PYTHON SOURCE LINES 16-17

Let's define the object and the BRDF

.. GENERATED FROM PYTHON SOURCE LINES 17-19

.. code-block:: Python

    obj = mr.SpaceObject("cube.obj")
    brdf = mr.Brdf("phong", cd=0.5, cs=0.5, n=10)







.. GENERATED FROM PYTHON SOURCE LINES 20-21

We now define the Multi-Layer Perceptron (MLP) brightness model. Note that the ``layers=(150, 50, 150)`` keyword argument defines the number of neurons in each densely-connected layer.

.. GENERATED FROM PYTHON SOURCE LINES 21-22

.. code-block:: Python

    mlp_bm = mrsim.MLPBrightnessModel(obj, brdf, use_engine=False)







.. GENERATED FROM PYTHON SOURCE LINES 23-24

Now we train the model on a set number of training lighting and observation configurations. Usually ``1e5``-``1e6`` are required for a *good* fit

.. GENERATED FROM PYTHON SOURCE LINES 24-27

.. code-block:: Python

    num_train = int(1e3)
    mlp_bm.train(num_train)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 1.39e-02 seconds
    Iteration 1, loss = 0.18410690
    Iteration 2, loss = 0.16347157
    Iteration 3, loss = 0.14609977
    Iteration 4, loss = 0.14807355
    Iteration 5, loss = 0.14403717
    Iteration 6, loss = 0.14078897
    Iteration 7, loss = 0.13904158
    Iteration 8, loss = 0.13769115
    Iteration 9, loss = 0.13732135
    Iteration 10, loss = 0.13615574
    Iteration 11, loss = 0.13550963
    Iteration 12, loss = 0.13344033
    Iteration 13, loss = 0.13097894
    Iteration 14, loss = 0.12883894
    Iteration 15, loss = 0.12669547
    Iteration 16, loss = 0.12539402
    Iteration 17, loss = 0.12325124
    Iteration 18, loss = 0.12139087
    Iteration 19, loss = 0.11802061
    Iteration 20, loss = 0.11566092
    Iteration 21, loss = 0.11567579
    Iteration 22, loss = 0.11084392
    Iteration 23, loss = 0.10527713
    Iteration 24, loss = 0.09973858
    Iteration 25, loss = 0.09578496
    Iteration 26, loss = 0.09171439
    Iteration 27, loss = 0.08750991
    Iteration 28, loss = 0.08547910
    Iteration 29, loss = 0.07960613
    Iteration 30, loss = 0.07417504
    Iteration 31, loss = 0.06827115
    Iteration 32, loss = 0.06424211
    Iteration 33, loss = 0.05974705
    Iteration 34, loss = 0.05913748
    Iteration 35, loss = 0.05382495
    Iteration 36, loss = 0.05035983
    Iteration 37, loss = 0.05064491
    Iteration 38, loss = 0.04756381
    Iteration 39, loss = 0.04702338
    Iteration 40, loss = 0.04217434
    Iteration 41, loss = 0.03992156
    Iteration 42, loss = 0.03825876
    Iteration 43, loss = 0.03861373
    Iteration 44, loss = 0.03658200
    Iteration 45, loss = 0.03409149
    Iteration 46, loss = 0.03278739
    Iteration 47, loss = 0.03047235
    Iteration 48, loss = 0.02837813
    Iteration 49, loss = 0.02781532
    Iteration 50, loss = 0.02757059
    Iteration 51, loss = 0.02869814
    Iteration 52, loss = 0.02755675
    Iteration 53, loss = 0.02607281
    Iteration 54, loss = 0.02581928
    Iteration 55, loss = 0.02345395
    Iteration 56, loss = 0.02343652
    Iteration 57, loss = 0.02019039
    Iteration 58, loss = 0.01909544
    Iteration 59, loss = 0.02000283
    Iteration 60, loss = 0.02243734
    Iteration 61, loss = 0.01955903
    Iteration 62, loss = 0.01967155
    Iteration 63, loss = 0.01664072
    Iteration 64, loss = 0.01625939
    Iteration 65, loss = 0.01595598
    Iteration 66, loss = 0.01468733
    Iteration 67, loss = 0.01400225
    Iteration 68, loss = 0.01346735
    Iteration 69, loss = 0.01248821
    Iteration 70, loss = 0.01209455
    Iteration 71, loss = 0.01174842
    Iteration 72, loss = 0.01231422
    Iteration 73, loss = 0.01119152
    Iteration 74, loss = 0.01094981
    Iteration 75, loss = 0.01077885
    Iteration 76, loss = 0.01170417
    Iteration 77, loss = 0.01133092
    Iteration 78, loss = 0.01046759
    Iteration 79, loss = 0.00967814
    Iteration 80, loss = 0.00995713
    Iteration 81, loss = 0.01042268
    Iteration 82, loss = 0.00925895
    Iteration 83, loss = 0.00878898
    Iteration 84, loss = 0.00815622
    Iteration 85, loss = 0.00808517
    Iteration 86, loss = 0.00789401
    Iteration 87, loss = 0.00728336
    Iteration 88, loss = 0.00709035
    Iteration 89, loss = 0.00764513
    Iteration 90, loss = 0.00809797
    Iteration 91, loss = 0.00871849
    Iteration 92, loss = 0.00675056
    Iteration 93, loss = 0.00758618
    Iteration 94, loss = 0.00710283
    Iteration 95, loss = 0.00672379
    Iteration 96, loss = 0.00678839
    Iteration 97, loss = 0.00626794
    Iteration 98, loss = 0.00655846
    Iteration 99, loss = 0.00547843
    Iteration 100, loss = 0.00534046
    Iteration 101, loss = 0.00531946
    Iteration 102, loss = 0.00454048
    Iteration 103, loss = 0.00483076
    Iteration 104, loss = 0.00482067
    Iteration 105, loss = 0.00458565
    Iteration 106, loss = 0.00450593
    Iteration 107, loss = 0.00532789
    Iteration 108, loss = 0.00500055
    Iteration 109, loss = 0.00499016
    Iteration 110, loss = 0.00513668
    Iteration 111, loss = 0.00455387
    Iteration 112, loss = 0.00431666
    Iteration 113, loss = 0.00424650
    Iteration 114, loss = 0.00407885
    Iteration 115, loss = 0.00365438
    Iteration 116, loss = 0.00353412
    Iteration 117, loss = 0.00351653
    Iteration 118, loss = 0.00332190
    Iteration 119, loss = 0.00324676
    Iteration 120, loss = 0.00295317
    Iteration 121, loss = 0.00293505
    Iteration 122, loss = 0.00300295
    Iteration 123, loss = 0.00277631
    Iteration 124, loss = 0.00263858
    Iteration 125, loss = 0.00258825
    Iteration 126, loss = 0.00245248
    Iteration 127, loss = 0.00253579
    Iteration 128, loss = 0.00232468
    Iteration 129, loss = 0.00272818
    Iteration 130, loss = 0.00260588
    Iteration 131, loss = 0.00247796
    Iteration 132, loss = 0.00241724
    Iteration 133, loss = 0.00234534
    Iteration 134, loss = 0.00243123
    Iteration 135, loss = 0.00218554
    Iteration 136, loss = 0.00211767
    Iteration 137, loss = 0.00219255
    Iteration 138, loss = 0.00210907
    Iteration 139, loss = 0.00210679
    Iteration 140, loss = 0.00250956
    Iteration 141, loss = 0.00259808
    Iteration 142, loss = 0.00224239
    Iteration 143, loss = 0.00215762
    Iteration 144, loss = 0.00195013
    Iteration 145, loss = 0.00207759
    Iteration 146, loss = 0.00188233
    Iteration 147, loss = 0.00179107
    Iteration 148, loss = 0.00173110
    Iteration 149, loss = 0.00183037
    Iteration 150, loss = 0.00184737
    Iteration 151, loss = 0.00168361
    Iteration 152, loss = 0.00179954
    Iteration 153, loss = 0.00161385
    Iteration 154, loss = 0.00184426
    Iteration 155, loss = 0.00167448
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 2.29e+01 seconds




.. GENERATED FROM PYTHON SOURCE LINES 28-29

We can now simulate a torque-free attitude profile to inspect the quality of the fit

.. GENERATED FROM PYTHON SOURCE LINES 29-40

.. code-block:: Python

    t_eval = np.linspace(0, 10, 1000)
    q, _ = mr.propagate_attitude_torque_free(
        np.array([0.0, 0.0, 0.0, 1.0]),
        np.array([1.0, 1.0, 1.0]),
        np.diag([1, 2, 3]),
        t_eval,
    )
    dcm = mr.quat_to_dcm(q)
    ovb = mr.stack_mat_mult_vec(dcm, np.array([[1, 0, 0]]))
    svb = mr.stack_mat_mult_vec(dcm, np.array([[0, 1, 0]]))








.. GENERATED FROM PYTHON SOURCE LINES 41-42

Evaluating the model in its two available formats - as a native ``scikit-learn`` model and as an Open Neural Network eXchange (ONNX) model

.. GENERATED FROM PYTHON SOURCE LINES 42-49

.. code-block:: Python

    mr.tic("Evaluate trained model with sklearn")
    mdl_b_sklearn = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    mr.toc()
    mr.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate trained model with sklearn: 3.86e-03 seconds
    Evaluate trained model with onnx: 2.90e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 50-51

We can save both of these representations to file:

.. GENERATED FROM PYTHON SOURCE LINES 51-54

.. code-block:: Python

    mlp_bm.save_to_file(save_as_format="onnx")
    mlp_bm.save_to_file(save_as_format="sklearn")








.. GENERATED FROM PYTHON SOURCE LINES 55-56

Now we load the model from its ``.onxx`` file we just saved and evaluate the brightness

.. GENERATED FROM PYTHON SOURCE LINES 56-61

.. code-block:: Python

    mlp_bm.load_from_file(mlp_bm.onnx_file_name)
    mr.tic("Evaluate loaded model with onxx")
    mdl_onnx_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with onxx: 2.11e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 62-63

And we do the same for the ``scikit-learn`` ``.plk`` file we saved

.. GENERATED FROM PYTHON SOURCE LINES 63-68

.. code-block:: Python

    mlp_bm.load_from_file(mlp_bm.sklearn_file_name)
    mr.tic("Evaluate loaded model with sklearn")
    mdl_sklearn_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with sklearn: 1.82e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 69-70

We can easily confirm that all four model evaluations have produced the same prediction

.. GENERATED FROM PYTHON SOURCE LINES 70-75

.. code-block:: Python

    print(np.max(np.abs(mdl_b_sklearn - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_sklearn - mdl_sklearn_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_sklearn_loaded)))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    9.6052970777194e-07
    0.0
    0.0
    9.6052970777194e-07




.. GENERATED FROM PYTHON SOURCE LINES 76-77

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 77-89

.. code-block:: Python

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_sklearn, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {num_train} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Normalized brightness")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()




.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001_2_00x.png 2.00x
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 90-91

We can also train on magnitude data instead of irradiance:

.. GENERATED FROM PYTHON SOURCE LINES 91-98

.. code-block:: Python

    mlp_bm = mrsim.MLPBrightnessModel(obj, brdf, use_engine=True)
    mlp_bm.train(num_train)

    mr.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb)
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 7.12e-01 seconds
    Iteration 1, loss = 0.27852116
    Iteration 2, loss = 0.16441285
    Iteration 3, loss = 0.14208105
    Iteration 4, loss = 0.12805352
    Iteration 5, loss = 0.12341624
    Iteration 6, loss = 0.12365512
    Iteration 7, loss = 0.12132460
    Iteration 8, loss = 0.11811605
    Iteration 9, loss = 0.11717749
    Iteration 10, loss = 0.11658074
    Iteration 11, loss = 0.11452002
    Iteration 12, loss = 0.11408015
    Iteration 13, loss = 0.11197520
    Iteration 14, loss = 0.11116258
    Iteration 15, loss = 0.10988223
    Iteration 16, loss = 0.10916067
    Iteration 17, loss = 0.10753898
    Iteration 18, loss = 0.10599859
    Iteration 19, loss = 0.10384161
    Iteration 20, loss = 0.10170052
    Iteration 21, loss = 0.10000887
    Iteration 22, loss = 0.09818593
    Iteration 23, loss = 0.09539782
    Iteration 24, loss = 0.09309890
    Iteration 25, loss = 0.09025061
    Iteration 26, loss = 0.08644522
    Iteration 27, loss = 0.08424207
    Iteration 28, loss = 0.08058180
    Iteration 29, loss = 0.07711482
    Iteration 30, loss = 0.07290872
    Iteration 31, loss = 0.06945918
    Iteration 32, loss = 0.06513171
    Iteration 33, loss = 0.06310697
    Iteration 34, loss = 0.05883166
    Iteration 35, loss = 0.05591795
    Iteration 36, loss = 0.05246905
    Iteration 37, loss = 0.04999980
    Iteration 38, loss = 0.05109136
    Iteration 39, loss = 0.04774316
    Iteration 40, loss = 0.04540844
    Iteration 41, loss = 0.04308856
    Iteration 42, loss = 0.04358154
    Iteration 43, loss = 0.03921406
    Iteration 44, loss = 0.03820323
    Iteration 45, loss = 0.03716883
    Iteration 46, loss = 0.03672709
    Iteration 47, loss = 0.03465897
    Iteration 48, loss = 0.03372821
    Iteration 49, loss = 0.03310365
    Iteration 50, loss = 0.03163168
    Iteration 51, loss = 0.03048838
    Iteration 52, loss = 0.03038164
    Iteration 53, loss = 0.02903500
    Iteration 54, loss = 0.02638238
    Iteration 55, loss = 0.02492411
    Iteration 56, loss = 0.02509822
    Iteration 57, loss = 0.02361348
    Iteration 58, loss = 0.02334551
    Iteration 59, loss = 0.02280365
    Iteration 60, loss = 0.02116850
    Iteration 61, loss = 0.02024949
    Iteration 62, loss = 0.02084713
    Iteration 63, loss = 0.02351057
    Iteration 64, loss = 0.02043784
    Iteration 65, loss = 0.01835374
    Iteration 66, loss = 0.01670979
    Iteration 67, loss = 0.01660582
    Iteration 68, loss = 0.01621323
    Iteration 69, loss = 0.01503566
    Iteration 70, loss = 0.01477517
    Iteration 71, loss = 0.01405142
    Iteration 72, loss = 0.01381924
    Iteration 73, loss = 0.01472239
    Iteration 74, loss = 0.01282519
    Iteration 75, loss = 0.01342410
    Iteration 76, loss = 0.01384318
    Iteration 77, loss = 0.01593974
    Iteration 78, loss = 0.01427524
    Iteration 79, loss = 0.01323042
    Iteration 80, loss = 0.01573253
    Iteration 81, loss = 0.01332944
    Iteration 82, loss = 0.01218225
    Iteration 83, loss = 0.01016916
    Iteration 84, loss = 0.01000232
    Iteration 85, loss = 0.00995649
    Iteration 86, loss = 0.00964956
    Iteration 87, loss = 0.00966318
    Iteration 88, loss = 0.00991157
    Iteration 89, loss = 0.01155577
    Iteration 90, loss = 0.00867391
    Iteration 91, loss = 0.00952827
    Iteration 92, loss = 0.00878244
    Iteration 93, loss = 0.00827190
    Iteration 94, loss = 0.00815723
    Iteration 95, loss = 0.00806310
    Iteration 96, loss = 0.00811386
    Iteration 97, loss = 0.00887145
    Iteration 98, loss = 0.00783406
    Iteration 99, loss = 0.00801869
    Iteration 100, loss = 0.00707343
    Iteration 101, loss = 0.00677268
    Iteration 102, loss = 0.00619997
    Iteration 103, loss = 0.00637237
    Iteration 104, loss = 0.00618094
    Iteration 105, loss = 0.00557148
    Iteration 106, loss = 0.00561376
    Iteration 107, loss = 0.00525683
    Iteration 108, loss = 0.00501641
    Iteration 109, loss = 0.00540699
    Iteration 110, loss = 0.00540267
    Iteration 111, loss = 0.00527918
    Iteration 112, loss = 0.00498523
    Iteration 113, loss = 0.00491570
    Iteration 114, loss = 0.00509349
    Iteration 115, loss = 0.00489757
    Iteration 116, loss = 0.00431367
    Iteration 117, loss = 0.00409182
    Iteration 118, loss = 0.00418249
    Iteration 119, loss = 0.00387209
    Iteration 120, loss = 0.00377751
    Iteration 121, loss = 0.00384210
    Iteration 122, loss = 0.00371356
    Iteration 123, loss = 0.00355379
    Iteration 124, loss = 0.00352145
    Iteration 125, loss = 0.00343166
    Iteration 126, loss = 0.00338746
    Iteration 127, loss = 0.00341544
    Iteration 128, loss = 0.00322153
    Iteration 129, loss = 0.00315718
    Iteration 130, loss = 0.00321141
    Iteration 131, loss = 0.00307912
    Iteration 132, loss = 0.00313511
    Iteration 133, loss = 0.00325599
    Iteration 134, loss = 0.00292889
    Iteration 135, loss = 0.00292317
    Iteration 136, loss = 0.00292657
    Iteration 137, loss = 0.00283530
    Iteration 138, loss = 0.00275090
    Iteration 139, loss = 0.00262533
    Iteration 140, loss = 0.00250167
    Iteration 141, loss = 0.00250405
    Iteration 142, loss = 0.00255924
    Iteration 143, loss = 0.00276825
    Iteration 144, loss = 0.00271193
    Iteration 145, loss = 0.00301527
    Iteration 146, loss = 0.00243267
    Iteration 147, loss = 0.00241649
    Iteration 148, loss = 0.00233365
    Iteration 149, loss = 0.00236287
    Iteration 150, loss = 0.00244390
    Iteration 151, loss = 0.00245653
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 3.33e+00 seconds
    Evaluate trained model with onnx: 1.16e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 99-100

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 100-111

.. code-block:: Python

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_onnx, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {num_train} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Apparent Magnitude")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()



.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002_2_00x.png 2.00x
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 28.451 seconds)


.. _sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: neural_network_brightness.ipynb <neural_network_brightness.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: neural_network_brightness.py <neural_network_brightness.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
