
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/01-light_curves/neural_network_brightness.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_01-light_curves_neural_network_brightness.py:


Neural Network Brightness
=========================

Trains a neural network to predict the brightness of a specular cube in an arbitrary lighting and observation conditions and compares the results to the truth

.. GENERATED FROM PYTHON SOURCE LINES 7-15

.. code-block:: default


    import matplotlib.pyplot as plt
    import numpy as np
    import seaborn as sns

    import mirage as mr
    import mirage.sim as mrsim








.. GENERATED FROM PYTHON SOURCE LINES 16-17

Let's define the object and the BRDF

.. GENERATED FROM PYTHON SOURCE LINES 17-19

.. code-block:: default

    obj = mr.SpaceObject("cube.obj")
    brdf = mr.Brdf("phong", cd=0.5, cs=0.5, n=10)







.. GENERATED FROM PYTHON SOURCE LINES 20-21

We now define the Multi-Layer Perceptron (MLP) brightness model. Note that the ``layers=(150, 50, 150)`` keyword argument defines the number of neurons in each densely-connected layer.

.. GENERATED FROM PYTHON SOURCE LINES 21-22

.. code-block:: default

    mlp_bm = mrsim.MLPBrightnessModel(obj, brdf, use_engine=False, train_on="irradiance")







.. GENERATED FROM PYTHON SOURCE LINES 23-24

Now we train the model on a set number of training lighting and observation configurations. Usually ``1e5``-``1e6`` are required for a *good* fit

.. GENERATED FROM PYTHON SOURCE LINES 24-27

.. code-block:: default

    num_train = int(1e3)
    mlp_bm.train(num_train)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 1.10e-03 seconds
    Iteration 1, loss = 0.03619536
    Iteration 2, loss = 0.02110437
    Iteration 3, loss = 0.01130093
    Iteration 4, loss = 0.01249740
    Iteration 5, loss = 0.01059090
    Iteration 6, loss = 0.01055470
    Iteration 7, loss = 0.01013532
    Iteration 8, loss = 0.00979367
    Iteration 9, loss = 0.00962374
    Iteration 10, loss = 0.00944574
    Iteration 11, loss = 0.00929891
    Iteration 12, loss = 0.00914132
    Iteration 13, loss = 0.00899019
    Iteration 14, loss = 0.00886320
    Iteration 15, loss = 0.00868861
    Iteration 16, loss = 0.00849836
    Iteration 17, loss = 0.00832534
    Iteration 18, loss = 0.00818719
    Iteration 19, loss = 0.00797260
    Iteration 20, loss = 0.00779581
    Iteration 21, loss = 0.00760832
    Iteration 22, loss = 0.00737185
    Iteration 23, loss = 0.00719166
    Iteration 24, loss = 0.00699464
    Iteration 25, loss = 0.00674752
    Iteration 26, loss = 0.00654742
    Iteration 27, loss = 0.00647435
    Iteration 28, loss = 0.00622051
    Iteration 29, loss = 0.00580734
    Iteration 30, loss = 0.00567244
    Iteration 31, loss = 0.00553056
    Iteration 32, loss = 0.00512543
    Iteration 33, loss = 0.00490544
    Iteration 34, loss = 0.00478820
    Iteration 35, loss = 0.00467924
    Iteration 36, loss = 0.00441521
    Iteration 37, loss = 0.00452486
    Iteration 38, loss = 0.00437649
    Iteration 39, loss = 0.00402263
    Iteration 40, loss = 0.00408596
    Iteration 41, loss = 0.00393589
    Iteration 42, loss = 0.00425387
    Iteration 43, loss = 0.00388728
    Iteration 44, loss = 0.00378452
    Iteration 45, loss = 0.00352130
    Iteration 46, loss = 0.00334429
    Iteration 47, loss = 0.00317366
    Iteration 48, loss = 0.00317501
    Iteration 49, loss = 0.00298919
    Iteration 50, loss = 0.00283048
    Iteration 51, loss = 0.00265469
    Iteration 52, loss = 0.00262484
    Iteration 53, loss = 0.00254334
    Iteration 54, loss = 0.00249179
    Iteration 55, loss = 0.00239720
    Iteration 56, loss = 0.00230214
    Iteration 57, loss = 0.00227549
    Iteration 58, loss = 0.00217069
    Iteration 59, loss = 0.00212997
    Iteration 60, loss = 0.00206576
    Iteration 61, loss = 0.00201930
    Iteration 62, loss = 0.00194153
    Iteration 63, loss = 0.00178338
    Iteration 64, loss = 0.00174694
    Iteration 65, loss = 0.00170321
    Iteration 66, loss = 0.00164498
    Iteration 67, loss = 0.00153594
    Iteration 68, loss = 0.00144403
    Iteration 69, loss = 0.00151472
    Iteration 70, loss = 0.00186775
    Iteration 71, loss = 0.00165152
    Iteration 72, loss = 0.00146064
    Iteration 73, loss = 0.00138958
    Iteration 74, loss = 0.00151542
    Iteration 75, loss = 0.00127884
    Iteration 76, loss = 0.00118427
    Iteration 77, loss = 0.00115981
    Iteration 78, loss = 0.00108982
    Iteration 79, loss = 0.00107468
    Iteration 80, loss = 0.00114299
    Iteration 81, loss = 0.00098748
    Iteration 82, loss = 0.00097665
    Iteration 83, loss = 0.00093433
    Iteration 84, loss = 0.00094794
    Iteration 85, loss = 0.00087597
    Iteration 86, loss = 0.00084348
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 2.61e+00 seconds
    Serialize model: 2.83e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 28-29

We can now simulate a torque-free attitude profile to inspect the quality of the fit

.. GENERATED FROM PYTHON SOURCE LINES 29-40

.. code-block:: default

    t_eval = np.linspace(0, 10, 1000)
    q, _ = mr.propagate_attitude_torque_free(
        np.array([0.0, 0.0, 0.0, 1.0]),
        np.array([1.0, 1.0, 1.0]),
        np.diag([1, 2, 3]),
        t_eval,
    )
    dcm = mr.quat_to_dcm(q)
    ovb = mr.stack_mat_mult_vec(dcm, np.array([[1, 0, 0]]))
    svb = mr.stack_mat_mult_vec(dcm, np.array([[0, 1, 0]]))








.. GENERATED FROM PYTHON SOURCE LINES 41-42

Evaluating the model in its two available formats - as a native ``scikit-learn`` model and as an Open Neural Network eXchange (ONNX) model

.. GENERATED FROM PYTHON SOURCE LINES 42-49

.. code-block:: default

    mr.tic("Evaluate trained model with sklearn")
    mdl_b_sklearn = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    mr.toc()
    mr.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate trained model with sklearn: 1.08e-02 seconds
    Evaluate trained model with onnx: 1.75e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 50-51

We can save both of these representations to file:

.. GENERATED FROM PYTHON SOURCE LINES 51-54

.. code-block:: default

    mlp_bm.save_to_file(save_as_format="onnx")
    mlp_bm.save_to_file(save_as_format="sklearn")








.. GENERATED FROM PYTHON SOURCE LINES 55-56

Now we load the model from its ``.onxx`` file we just saved and evaluate the brightness

.. GENERATED FROM PYTHON SOURCE LINES 56-61

.. code-block:: default

    mlp_bm.load_from_file(mlp_bm.onnx_file_name)
    mr.tic("Evaluate loaded model with onxx")
    mdl_onnx_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with onxx: 1.15e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 62-63

And we do the same for the ``scikit-learn`` ``.plk`` file we saved

.. GENERATED FROM PYTHON SOURCE LINES 63-68

.. code-block:: default

    mlp_bm.load_from_file(mlp_bm.sklearn_file_name)
    mr.tic("Evaluate loaded model with sklearn")
    mdl_sklearn_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with sklearn: 1.08e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 69-70

We can easily confirm that all four model evaluations have produced the same prediction

.. GENERATED FROM PYTHON SOURCE LINES 70-75

.. code-block:: default

    print(np.max(np.abs(mdl_b_sklearn - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_sklearn - mdl_sklearn_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_sklearn_loaded)))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    9.89226502934315e-07
    0.0
    0.0
    9.89226502934315e-07




.. GENERATED FROM PYTHON SOURCE LINES 76-77

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 77-89

.. code-block:: default

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_sklearn, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {num_train} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Normalized brightness")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()




.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001_2_00x.png 2.00x
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 90-91

We can also train on magnitude data instead of irradiance:

.. GENERATED FROM PYTHON SOURCE LINES 91-98

.. code-block:: default

    mlp_bm = mrsim.MLPBrightnessModel(obj, brdf, use_engine=True, train_on="magnitude")
    mlp_bm.train(num_train)

    mr.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb)
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 6.14e-01 seconds
    Iteration 1, loss = 0.02166121
    Iteration 2, loss = 0.01335950
    Iteration 3, loss = 0.01219952
    Iteration 4, loss = 0.01113278
    Iteration 5, loss = 0.01044459
    Iteration 6, loss = 0.01003856
    Iteration 7, loss = 0.00984952
    Iteration 8, loss = 0.00952799
    Iteration 9, loss = 0.00915139
    Iteration 10, loss = 0.00896227
    Iteration 11, loss = 0.00887376
    Iteration 12, loss = 0.00910756
    Iteration 13, loss = 0.00903966
    Iteration 14, loss = 0.00951822
    Iteration 15, loss = 0.00851596
    Iteration 16, loss = 0.00814789
    Iteration 17, loss = 0.00763806
    Iteration 18, loss = 0.00739278
    Iteration 19, loss = 0.00734643
    Iteration 20, loss = 0.00671750
    Iteration 21, loss = 0.00674923
    Iteration 22, loss = 0.00643367
    Iteration 23, loss = 0.00622400
    Iteration 24, loss = 0.00589798
    Iteration 25, loss = 0.00565895
    Iteration 26, loss = 0.00542103
    Iteration 27, loss = 0.00541499
    Iteration 28, loss = 0.00548408
    Iteration 29, loss = 0.00502752
    Iteration 30, loss = 0.00480306
    Iteration 31, loss = 0.00478449
    Iteration 32, loss = 0.00436224
    Iteration 33, loss = 0.00423652
    Iteration 34, loss = 0.00414352
    Iteration 35, loss = 0.00417173
    Iteration 36, loss = 0.00401915
    Iteration 37, loss = 0.00367601
    Iteration 38, loss = 0.00365356
    Iteration 39, loss = 0.00346309
    Iteration 40, loss = 0.00354027
    Iteration 41, loss = 0.00324313
    Iteration 42, loss = 0.00322793
    Iteration 43, loss = 0.00329541
    Iteration 44, loss = 0.00315323
    Iteration 45, loss = 0.00290844
    Iteration 46, loss = 0.00270386
    Iteration 47, loss = 0.00274353
    Iteration 48, loss = 0.00269166
    Iteration 49, loss = 0.00260868
    Iteration 50, loss = 0.00262454
    Iteration 51, loss = 0.00242571
    Iteration 52, loss = 0.00229876
    Iteration 53, loss = 0.00227481
    Iteration 54, loss = 0.00234464
    Iteration 55, loss = 0.00269706
    Iteration 56, loss = 0.00250656
    Iteration 57, loss = 0.00247355
    Iteration 58, loss = 0.00206336
    Iteration 59, loss = 0.00193567
    Iteration 60, loss = 0.00190067
    Iteration 61, loss = 0.00192030
    Iteration 62, loss = 0.00175424
    Iteration 63, loss = 0.00195925
    Iteration 64, loss = 0.00193279
    Iteration 65, loss = 0.00200184
    Iteration 66, loss = 0.00190787
    Iteration 67, loss = 0.00179210
    Iteration 68, loss = 0.00174659
    Iteration 69, loss = 0.00171215
    Iteration 70, loss = 0.00162754
    Iteration 71, loss = 0.00155178
    Iteration 72, loss = 0.00151952
    Iteration 73, loss = 0.00152617
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 1.78e+00 seconds
    Serialize model: 2.50e-02 seconds
    Evaluate trained model with onnx: 4.66e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 99-100

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 100-111

.. code-block:: default

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_onnx, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {num_train} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Apparent Magnitude")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()



.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002_2_00x.png 2.00x
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  6.385 seconds)


.. _sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: neural_network_brightness.py <neural_network_brightness.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: neural_network_brightness.ipynb <neural_network_brightness.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
