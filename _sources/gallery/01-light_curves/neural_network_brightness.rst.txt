
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/01-light_curves/neural_network_brightness.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_01-light_curves_neural_network_brightness.py:


Neural Network Brightness
=========================

Trains a neural network to predict the brightness of a specular cube in an arbitrary lighting and observation conditions and compares the results to the truth

.. GENERATED FROM PYTHON SOURCE LINES 7-16

.. code-block:: default



    import matplotlib.pyplot as plt
    import numpy as np
    import seaborn as sns

    import pyspaceaware as ps
    import pyspaceaware.sim as pssim








.. GENERATED FROM PYTHON SOURCE LINES 17-18

Let's define the object and the BRDF

.. GENERATED FROM PYTHON SOURCE LINES 18-20

.. code-block:: default

    obj = ps.SpaceObject("cube.obj")
    brdf = ps.Brdf("phong", cd=0.5, cs=0.5, n=10)







.. GENERATED FROM PYTHON SOURCE LINES 21-22

We now define the Multi-Layer Perceptron (MLP) brightness model. Note that the ``layers=(150, 50, 150)`` keyword argument defines the number of neurons in each densely-connected layer.

.. GENERATED FROM PYTHON SOURCE LINES 22-23

.. code-block:: default

    mlp_bm = pssim.MLPBrightnessModel(obj, brdf, use_engine=False, train_on="irradiance")







.. GENERATED FROM PYTHON SOURCE LINES 24-25

Now we train the model on a set number of training lighting and observation configurations. Usually ``1e5``-``1e6`` are required for a *good* fit

.. GENERATED FROM PYTHON SOURCE LINES 25-28

.. code-block:: default

    num_train = int(1e3)
    mlp_bm.train(num_train)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 1.22e-03 seconds
    (1024, 6) (1024,)
    Iteration 1, loss = 0.19883757
    Iteration 2, loss = 0.14378746
    Iteration 3, loss = 0.13600742
    Iteration 4, loss = 0.13192813
    Iteration 5, loss = 0.12723580
    Iteration 6, loss = 0.12271236
    Iteration 7, loss = 0.11897731
    Iteration 8, loss = 0.11589987
    Iteration 9, loss = 0.11249839
    Iteration 10, loss = 0.10972333
    Iteration 11, loss = 0.10845376
    Iteration 12, loss = 0.10331442
    Iteration 13, loss = 0.10363721
    Iteration 14, loss = 0.09878451
    Iteration 15, loss = 0.09541999
    Iteration 16, loss = 0.09440810
    Iteration 17, loss = 0.09078280
    Iteration 18, loss = 0.08425442
    Iteration 19, loss = 0.08236091
    Iteration 20, loss = 0.07741269
    Iteration 21, loss = 0.07310391
    Iteration 22, loss = 0.06889315
    Iteration 23, loss = 0.06511388
    Iteration 24, loss = 0.06373840
    Iteration 25, loss = 0.05980829
    Iteration 26, loss = 0.05643388
    Iteration 27, loss = 0.05405120
    Iteration 28, loss = 0.05106054
    Iteration 29, loss = 0.04803296
    Iteration 30, loss = 0.04705927
    Iteration 31, loss = 0.04536691
    Iteration 32, loss = 0.04321787
    Iteration 33, loss = 0.04014272
    Iteration 34, loss = 0.03858356
    Iteration 35, loss = 0.03860961
    Iteration 36, loss = 0.03626298
    Iteration 37, loss = 0.03615115
    Iteration 38, loss = 0.03358105
    Iteration 39, loss = 0.03219782
    Iteration 40, loss = 0.03159506
    Iteration 41, loss = 0.02965365
    Iteration 42, loss = 0.02822961
    Iteration 43, loss = 0.02670210
    Iteration 44, loss = 0.02820396
    Iteration 45, loss = 0.02733654
    Iteration 46, loss = 0.02470955
    Iteration 47, loss = 0.02406819
    Iteration 48, loss = 0.02394789
    Iteration 49, loss = 0.02310331
    Iteration 50, loss = 0.02194177
    Iteration 51, loss = 0.02059306
    Iteration 52, loss = 0.02042300
    Iteration 53, loss = 0.02022862
    Iteration 54, loss = 0.01999247
    Iteration 55, loss = 0.01865854
    Iteration 56, loss = 0.01812332
    Iteration 57, loss = 0.01614900
    Iteration 58, loss = 0.01591887
    Iteration 59, loss = 0.01565454
    Iteration 60, loss = 0.01479828
    Iteration 61, loss = 0.01379698
    Iteration 62, loss = 0.01312796
    Iteration 63, loss = 0.01351767
    Iteration 64, loss = 0.01388027
    Iteration 65, loss = 0.01279462
    Iteration 66, loss = 0.01134011
    Iteration 67, loss = 0.01118441
    Iteration 68, loss = 0.01066786
    Iteration 69, loss = 0.01111345
    Iteration 70, loss = 0.01076868
    Iteration 71, loss = 0.00993869
    Iteration 72, loss = 0.01008985
    Iteration 73, loss = 0.01046137
    Iteration 74, loss = 0.00979298
    Iteration 75, loss = 0.00972430
    Iteration 76, loss = 0.00892210
    Iteration 77, loss = 0.00813654
    Iteration 78, loss = 0.00869991
    Iteration 79, loss = 0.00871470
    Iteration 80, loss = 0.01024509
    Iteration 81, loss = 0.01008041
    Iteration 82, loss = 0.01052447
    Iteration 83, loss = 0.00860500
    Iteration 84, loss = 0.00883405
    Iteration 85, loss = 0.00788491
    Iteration 86, loss = 0.00792084
    Iteration 87, loss = 0.00662627
    Iteration 88, loss = 0.00666070
    Iteration 89, loss = 0.00600197
    Iteration 90, loss = 0.00612079
    Iteration 91, loss = 0.00571623
    Iteration 92, loss = 0.00522334
    Iteration 93, loss = 0.00531737
    Iteration 94, loss = 0.00483913
    Iteration 95, loss = 0.00494008
    Iteration 96, loss = 0.00491656
    Iteration 97, loss = 0.00472289
    Iteration 98, loss = 0.00493774
    Iteration 99, loss = 0.00536703
    Iteration 100, loss = 0.00587789
    Iteration 101, loss = 0.00458315
    Iteration 102, loss = 0.00444503
    Iteration 103, loss = 0.00392410
    Iteration 104, loss = 0.00454404
    Iteration 105, loss = 0.00442662
    Iteration 106, loss = 0.00412489
    Iteration 107, loss = 0.00398563
    Iteration 108, loss = 0.00384519
    Iteration 109, loss = 0.00356898
    Iteration 110, loss = 0.00374535
    Iteration 111, loss = 0.00369987
    Iteration 112, loss = 0.00369645
    Iteration 113, loss = 0.00387064
    Iteration 114, loss = 0.00333573
    Iteration 115, loss = 0.00342204
    Iteration 116, loss = 0.00277256
    Iteration 117, loss = 0.00285980
    Iteration 118, loss = 0.00282010
    Iteration 119, loss = 0.00277404
    Iteration 120, loss = 0.00289515
    Iteration 121, loss = 0.00290727
    Iteration 122, loss = 0.00263731
    Iteration 123, loss = 0.00262476
    Iteration 124, loss = 0.00276251
    Iteration 125, loss = 0.00247093
    Iteration 126, loss = 0.00226366
    Iteration 127, loss = 0.00224361
    Iteration 128, loss = 0.00220082
    Iteration 129, loss = 0.00300292
    Iteration 130, loss = 0.00284594
    Iteration 131, loss = 0.00278865
    Iteration 132, loss = 0.00277317
    Iteration 133, loss = 0.00244661
    Iteration 134, loss = 0.00227264
    Iteration 135, loss = 0.00200569
    Iteration 136, loss = 0.00198847
    Iteration 137, loss = 0.00185976
    Iteration 138, loss = 0.00192243
    Iteration 139, loss = 0.00199712
    Iteration 140, loss = 0.00168588
    Iteration 141, loss = 0.00168525
    Iteration 142, loss = 0.00177319
    Iteration 143, loss = 0.00160438
    Iteration 144, loss = 0.00155751
    Iteration 145, loss = 0.00148633
    Iteration 146, loss = 0.00146046
    Iteration 147, loss = 0.00145918
    Iteration 148, loss = 0.00157539
    Iteration 149, loss = 0.00172299
    Iteration 150, loss = 0.00154266
    Iteration 151, loss = 0.00165810
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 8.39e+00 seconds
    Serialize model: 6.91e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 29-30

We can now simulate a torque-free attitude profile to inspect the quality of the fit

.. GENERATED FROM PYTHON SOURCE LINES 30-41

.. code-block:: default

    t_eval = np.linspace(0, 10, 1000)
    q, _ = ps.propagate_attitude_torque_free(
        np.array([0.0, 0.0, 0.0, 1.0]),
        np.array([1.0, 1.0, 1.0]),
        np.diag([1, 2, 3]),
        t_eval,
    )
    dcm = ps.quat_to_dcm(q)
    ovb = ps.stack_mat_mult_vec(dcm, np.array([[1, 0, 0]]))
    svb = ps.stack_mat_mult_vec(dcm, np.array([[0, 1, 0]]))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [1. 1. 1.] False




.. GENERATED FROM PYTHON SOURCE LINES 42-43

Evaluating the model in its two available formats - as a native ``scikit-learn`` model and as an Open Neural Network eXchange (ONNX) model

.. GENERATED FROM PYTHON SOURCE LINES 43-50

.. code-block:: default

    ps.tic("Evaluate trained model with sklearn")
    mdl_b_sklearn = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    ps.toc()
    ps.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    ps.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate trained model with sklearn: 1.06e-02 seconds
    Evaluate trained model with onnx: 4.43e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 51-52

We can save both of these representations to file:

.. GENERATED FROM PYTHON SOURCE LINES 52-55

.. code-block:: default

    mlp_bm.save_to_file(save_as_format="onnx")
    mlp_bm.save_to_file(save_as_format="sklearn")








.. GENERATED FROM PYTHON SOURCE LINES 56-57

Now we load the model from its ``.onxx`` file we just saved and evaluate the brightness

.. GENERATED FROM PYTHON SOURCE LINES 57-62

.. code-block:: default

    mlp_bm.load_from_file(mlp_bm.onnx_file_name)
    ps.tic("Evaluate loaded model with onxx")
    mdl_onnx_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    ps.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with onxx: 1.72e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 63-64

And we do the same for the ``scikit-learn`` ``.plk`` file we saved

.. GENERATED FROM PYTHON SOURCE LINES 64-69

.. code-block:: default

    mlp_bm.load_from_file(mlp_bm.sklearn_file_name)
    ps.tic("Evaluate loaded model with sklearn")
    mdl_sklearn_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    ps.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with sklearn: 2.13e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 70-71

We can easily confirm that all four model evaluations have produced the same prediction

.. GENERATED FROM PYTHON SOURCE LINES 71-76

.. code-block:: default

    print(np.max(np.abs(mdl_b_sklearn - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_sklearn - mdl_sklearn_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_sklearn_loaded)))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    1.1107622606143508e-06
    0.0
    0.0
    1.1107622606143508e-06




.. GENERATED FROM PYTHON SOURCE LINES 77-78

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 78-90

.. code-block:: default

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_sklearn, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {num_train} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Normalized brightness")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()




.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001_2_00x.png 2.00x
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 91-92

We can also train on magnitude data instead of irradiance:

.. GENERATED FROM PYTHON SOURCE LINES 92-99

.. code-block:: default

    mlp_bm = pssim.MLPBrightnessModel(obj, brdf, use_engine=True, train_on="magnitude")
    mlp_bm.train(num_train)

    ps.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb)
    ps.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 6.46e-01 seconds
    (1024, 6) (1024,)
    Iteration 1, loss = 142.32307331
    Iteration 2, loss = 130.55406989
    Iteration 3, loss = 114.98980943
    Iteration 4, loss = 92.45873330
    Iteration 5, loss = 61.57774679
    Iteration 6, loss = 28.32620134
    Iteration 7, loss = 9.98108866
    Iteration 8, loss = 14.00012649
    Iteration 9, loss = 10.83898233
    Iteration 10, loss = 7.85517055
    Iteration 11, loss = 7.84052973
    Iteration 12, loss = 7.19665913
    Iteration 13, loss = 6.64895528
    Iteration 14, loss = 6.46308281
    Iteration 15, loss = 6.11155498
    Iteration 16, loss = 5.94608027
    Iteration 17, loss = 5.75815944
    Iteration 18, loss = 5.60782514
    Iteration 19, loss = 5.48867438
    Iteration 20, loss = 5.37420803
    Iteration 21, loss = 5.27331222
    Iteration 22, loss = 5.21103453
    Iteration 23, loss = 5.13856129
    Iteration 24, loss = 5.10701060
    Iteration 25, loss = 5.04825457
    Iteration 26, loss = 4.99015126
    Iteration 27, loss = 4.94004474
    Iteration 28, loss = 4.90200398
    Iteration 29, loss = 4.87695419
    Iteration 30, loss = 4.85041840
    Iteration 31, loss = 4.83602206
    Iteration 32, loss = 4.83956858
    Iteration 33, loss = 4.82172480
    Iteration 34, loss = 4.79380204
    Iteration 35, loss = 4.78962938
    Iteration 36, loss = 4.73657175
    Iteration 37, loss = 4.76007738
    Iteration 38, loss = 4.72077997
    Iteration 39, loss = 4.68834864
    Iteration 40, loss = 4.66806857
    Iteration 41, loss = 4.66968689
    Iteration 42, loss = 4.63928879
    Iteration 43, loss = 4.61481820
    Iteration 44, loss = 4.59585170
    Iteration 45, loss = 4.57101771
    Iteration 46, loss = 4.55297521
    Iteration 47, loss = 4.53331610
    Iteration 48, loss = 4.50933074
    Iteration 49, loss = 4.49408435
    Iteration 50, loss = 4.47359820
    Iteration 51, loss = 4.47285096
    Iteration 52, loss = 4.43042757
    Iteration 53, loss = 4.44596423
    Iteration 54, loss = 4.38224870
    Iteration 55, loss = 4.35695775
    Iteration 56, loss = 4.33996580
    Iteration 57, loss = 4.32029450
    Iteration 58, loss = 4.28434916
    Iteration 59, loss = 4.26359801
    Iteration 60, loss = 4.23438479
    Iteration 61, loss = 4.22236201
    Iteration 62, loss = 4.18343271
    Iteration 63, loss = 4.16955225
    Iteration 64, loss = 4.13805784
    Iteration 65, loss = 4.11416636
    Iteration 66, loss = 4.10372025
    Iteration 67, loss = 4.08122984
    Iteration 68, loss = 4.04780806
    Iteration 69, loss = 4.02354322
    Iteration 70, loss = 3.98075406
    Iteration 71, loss = 3.97463478
    Iteration 72, loss = 3.95695260
    Iteration 73, loss = 3.97136176
    Iteration 74, loss = 4.00105962
    Iteration 75, loss = 3.94070680
    Iteration 76, loss = 3.89207132
    Iteration 77, loss = 3.83819995
    Iteration 78, loss = 3.83987329
    Iteration 79, loss = 3.80132545
    Iteration 80, loss = 3.76738487
    Iteration 81, loss = 3.76046277
    Iteration 82, loss = 3.73292567
    Iteration 83, loss = 3.68906116
    Iteration 84, loss = 3.67401922
    Iteration 85, loss = 3.65331098
    Iteration 86, loss = 3.64239072
    Iteration 87, loss = 3.59959851
    Iteration 88, loss = 3.59587115
    Iteration 89, loss = 3.55753235
    Iteration 90, loss = 3.52734466
    Iteration 91, loss = 3.50774020
    Iteration 92, loss = 3.50544061
    Iteration 93, loss = 3.49682168
    Iteration 94, loss = 3.46701228
    Iteration 95, loss = 3.41236789
    Iteration 96, loss = 3.38820945
    Iteration 97, loss = 3.36908234
    Iteration 98, loss = 3.34187780
    Iteration 99, loss = 3.30801315
    Iteration 100, loss = 3.31252623
    Iteration 101, loss = 3.28671166
    Iteration 102, loss = 3.25898432
    Iteration 103, loss = 3.23172910
    Iteration 104, loss = 3.22521638
    Iteration 105, loss = 3.22983296
    Iteration 106, loss = 3.19646834
    Iteration 107, loss = 3.20060378
    Iteration 108, loss = 3.17409882
    Iteration 109, loss = 3.16157821
    Iteration 110, loss = 3.14032329
    Iteration 111, loss = 3.10492197
    Iteration 112, loss = 3.06613849
    Iteration 113, loss = 3.06489538
    Iteration 114, loss = 3.07792089
    Iteration 115, loss = 3.04339665
    Iteration 116, loss = 3.01958940
    Iteration 117, loss = 3.02269801
    Iteration 118, loss = 2.98299297
    Iteration 119, loss = 2.97985790
    Iteration 120, loss = 2.93204626
    Iteration 121, loss = 2.92805403
    Iteration 122, loss = 2.91665128
    Iteration 123, loss = 2.88272059
    Iteration 124, loss = 2.94418586
    Iteration 125, loss = 2.93224452
    Iteration 126, loss = 2.93145167
    Iteration 127, loss = 2.83728723
    Iteration 128, loss = 2.81452314
    Iteration 129, loss = 2.83821799
    Iteration 130, loss = 2.84019350
    Iteration 131, loss = 2.78917655
    Iteration 132, loss = 2.79324138
    Iteration 133, loss = 2.78889241
    Iteration 134, loss = 2.74426038
    Iteration 135, loss = 2.76422527
    Iteration 136, loss = 2.70987291
    Iteration 137, loss = 2.67915060
    Iteration 138, loss = 2.69944511
    Iteration 139, loss = 2.65584915
    Iteration 140, loss = 2.63436326
    Iteration 141, loss = 2.61715872
    Iteration 142, loss = 2.60138972
    Iteration 143, loss = 2.60014322
    Iteration 144, loss = 2.60585840
    Iteration 145, loss = 2.57762884
    Iteration 146, loss = 2.58280715
    Iteration 147, loss = 2.55610470
    Iteration 148, loss = 2.54390353
    Iteration 149, loss = 2.49411095
    Iteration 150, loss = 2.54463510
    Iteration 151, loss = 2.49691362
    Iteration 152, loss = 2.47781557
    Iteration 153, loss = 2.47778328
    Iteration 154, loss = 2.49047841
    Iteration 155, loss = 2.45119014
    Iteration 156, loss = 2.48103794
    Iteration 157, loss = 2.45995944
    Iteration 158, loss = 2.44927973
    Iteration 159, loss = 2.42108892
    Iteration 160, loss = 2.36557086
    Iteration 161, loss = 2.34204991
    Iteration 162, loss = 2.33646156
    Iteration 163, loss = 2.33279855
    Iteration 164, loss = 2.33083231
    Iteration 165, loss = 2.30835856
    Iteration 166, loss = 2.33834706
    Iteration 167, loss = 2.31936974
    Iteration 168, loss = 2.28162367
    Iteration 169, loss = 2.26702994
    Iteration 170, loss = 2.25137749
    Iteration 171, loss = 2.20834690
    Iteration 172, loss = 2.24595547
    Iteration 173, loss = 2.17991591
    Iteration 174, loss = 2.19397522
    Iteration 175, loss = 2.16995331
    Iteration 176, loss = 2.13399106
    Iteration 177, loss = 2.14556523
    Iteration 178, loss = 2.16464519
    Iteration 179, loss = 2.13906069
    Iteration 180, loss = 2.12123437
    Iteration 181, loss = 2.12070564
    Iteration 182, loss = 2.10487761
    Iteration 183, loss = 2.06035956
    Iteration 184, loss = 2.05470893
    Iteration 185, loss = 2.06728191
    Iteration 186, loss = 2.03991070
    Iteration 187, loss = 2.00949790
    Iteration 188, loss = 2.02967395
    Iteration 189, loss = 1.99159927
    Iteration 190, loss = 1.99700302
    Iteration 191, loss = 2.00683188
    Iteration 192, loss = 1.95651868
    Iteration 193, loss = 1.97031036
    Iteration 194, loss = 1.93399648
    Iteration 195, loss = 1.90484987
    Iteration 196, loss = 1.88824460
    Iteration 197, loss = 1.91374137
    Iteration 198, loss = 1.87843786
    Iteration 199, loss = 1.95554197
    Iteration 200, loss = 1.94059095
    /Users/liamrobinson/Documents/PyLightCurves/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
      warnings.warn(
    Fit against 1000 pts: : 1.54e+01 seconds
    Serialize model: 4.17e-02 seconds
    Evaluate trained model with onnx: 5.79e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 100-101

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 101-112

.. code-block:: default

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_onnx, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {num_train} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Apparent Magnitude")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()



.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002_2_00x.png 2.00x
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  25.825 seconds)


.. _sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: neural_network_brightness.py <neural_network_brightness.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: neural_network_brightness.ipynb <neural_network_brightness.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
