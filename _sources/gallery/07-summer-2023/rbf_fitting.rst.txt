
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/07-summer-2023/rbf_fitting.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_07-summer-2023_rbf_fitting.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_07-summer-2023_rbf_fitting.py:


Neural Network Brightness
=========================

Trains a neural network to predict the brightness of a specular cube in an arbitrary lighting and observation conditions and compares the results to the truth

.. GENERATED FROM PYTHON SOURCE LINES 7-16

.. code-block:: default


    import sys

    sys.path.append("./src")
    import numpy as np
    import pyspaceaware as ps
    import matplotlib.pyplot as plt
    import seaborn as sns








.. GENERATED FROM PYTHON SOURCE LINES 17-18

Let's define the object and the BRDF

.. GENERATED FROM PYTHON SOURCE LINES 18-20

.. code-block:: default

    obj = ps.SpaceObject("cube.obj")
    brdf = ps.Brdf("phong", cd=0.5, cs=0.5, n=10)







.. GENERATED FROM PYTHON SOURCE LINES 21-22

We now define the Multi-Layer Perceptron (MLP) brightness model. Note that the ``layers=(150, 50, 150)`` keyword argument defines the number of neurons in each densely-connected layer.

.. GENERATED FROM PYTHON SOURCE LINES 22-23

.. code-block:: default

    mlp_bm = ps.MLPBrightnessModel(obj, brdf, use_engine=False)







.. GENERATED FROM PYTHON SOURCE LINES 24-25

Now we train the model on a set number of training lighting and observation configurations. Usually ``1e5``-``1e6`` are required for a *good* fit

.. GENERATED FROM PYTHON SOURCE LINES 25-28

.. code-block:: default

    num_train = int(1e3)
    mlp_bm.train(num_train=num_train)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /Users/liamrobinson/Documents/PyLightCurves/src/pyspaceaware/lighting.py:48: RuntimeWarning: invalid value encountered in divide
      fs = cs * (n + 1) / (2 * np.pi) * rdot(R, O) ** n / NdL
    Compute training LC: 2.88e-03 seconds
    Iteration 1, loss = 0.18774222
    Iteration 2, loss = 0.15324416
    Iteration 3, loss = 0.13543048
    Iteration 4, loss = 0.13446600
    Iteration 5, loss = 0.12605517
    Iteration 6, loss = 0.12319239
    Iteration 7, loss = 0.11897641
    Iteration 8, loss = 0.11686532
    Iteration 9, loss = 0.11366989
    Iteration 10, loss = 0.11116266
    Iteration 11, loss = 0.10929433
    Iteration 12, loss = 0.10593984
    Iteration 13, loss = 0.10337925
    Iteration 14, loss = 0.10022938
    Iteration 15, loss = 0.09782853
    Iteration 16, loss = 0.09642232
    Iteration 17, loss = 0.09404759
    Iteration 18, loss = 0.08964537
    Iteration 19, loss = 0.08579700
    Iteration 20, loss = 0.08286149
    Iteration 21, loss = 0.07855742
    Iteration 22, loss = 0.07480672
    Iteration 23, loss = 0.06964270
    Iteration 24, loss = 0.06745349
    Iteration 25, loss = 0.06228476
    Iteration 26, loss = 0.05872266
    Iteration 27, loss = 0.05658078
    Iteration 28, loss = 0.05222204
    Iteration 29, loss = 0.05094390
    Iteration 30, loss = 0.05050217
    Iteration 31, loss = 0.04613178
    Iteration 32, loss = 0.04381322
    Iteration 33, loss = 0.04148543
    Iteration 34, loss = 0.04157464
    Iteration 35, loss = 0.03952520
    Iteration 36, loss = 0.03741160
    Iteration 37, loss = 0.03641320
    Iteration 38, loss = 0.03744549
    Iteration 39, loss = 0.03473975
    Iteration 40, loss = 0.03411813
    Iteration 41, loss = 0.03071803
    Iteration 42, loss = 0.03100968
    Iteration 43, loss = 0.02992572
    Iteration 44, loss = 0.02673110
    Iteration 45, loss = 0.02672399
    Iteration 46, loss = 0.02519214
    Iteration 47, loss = 0.02367260
    Iteration 48, loss = 0.02207933
    Iteration 49, loss = 0.02069449
    Iteration 50, loss = 0.01978055
    Iteration 51, loss = 0.01940418
    Iteration 52, loss = 0.02051941
    Iteration 53, loss = 0.01951438
    Iteration 54, loss = 0.01879431
    Iteration 55, loss = 0.01700520
    Iteration 56, loss = 0.01541692
    Iteration 57, loss = 0.01523817
    Iteration 58, loss = 0.01504424
    Iteration 59, loss = 0.01447218
    Iteration 60, loss = 0.01410180
    Iteration 61, loss = 0.01263857
    Iteration 62, loss = 0.01224513
    Iteration 63, loss = 0.01205522
    Iteration 64, loss = 0.01175556
    Iteration 65, loss = 0.01243286
    Iteration 66, loss = 0.01192899
    Iteration 67, loss = 0.01169406
    Iteration 68, loss = 0.01023293
    Iteration 69, loss = 0.01046588
    Iteration 70, loss = 0.00976087
    Iteration 71, loss = 0.01034684
    Iteration 72, loss = 0.00889451
    Iteration 73, loss = 0.00912081
    Iteration 74, loss = 0.00865496
    Iteration 75, loss = 0.00798171
    Iteration 76, loss = 0.00746062
    Iteration 77, loss = 0.00733258
    Iteration 78, loss = 0.00679631
    Iteration 79, loss = 0.00641806
    Iteration 80, loss = 0.00695642
    Iteration 81, loss = 0.00728598
    Iteration 82, loss = 0.00682302
    Iteration 83, loss = 0.00700397
    Iteration 84, loss = 0.00746141
    Iteration 85, loss = 0.00702924
    Iteration 86, loss = 0.00633443
    Iteration 87, loss = 0.00559587
    Iteration 88, loss = 0.00546087
    Iteration 89, loss = 0.00606288
    Iteration 90, loss = 0.00551222
    Iteration 91, loss = 0.00492960
    Iteration 92, loss = 0.00497260
    Iteration 93, loss = 0.00520899
    Iteration 94, loss = 0.00511297
    Iteration 95, loss = 0.00492487
    Iteration 96, loss = 0.00479248
    Iteration 97, loss = 0.00452820
    Iteration 98, loss = 0.00425425
    Iteration 99, loss = 0.00384369
    Iteration 100, loss = 0.00378233
    Iteration 101, loss = 0.00371302
    Iteration 102, loss = 0.00354938
    Iteration 103, loss = 0.00353461
    Iteration 104, loss = 0.00341815
    Iteration 105, loss = 0.00384953
    Iteration 106, loss = 0.00341347
    Iteration 107, loss = 0.00320110
    Iteration 108, loss = 0.00321307
    Iteration 109, loss = 0.00327607
    Iteration 110, loss = 0.00296173
    Iteration 111, loss = 0.00292938
    Iteration 112, loss = 0.00297904
    Iteration 113, loss = 0.00272970
    Iteration 114, loss = 0.00275413
    Iteration 115, loss = 0.00282040
    Iteration 116, loss = 0.00335087
    Iteration 117, loss = 0.00312735
    Iteration 118, loss = 0.00290171
    Iteration 119, loss = 0.00244897
    Iteration 120, loss = 0.00231949
    Iteration 121, loss = 0.00232882
    Iteration 122, loss = 0.00272240
    Iteration 123, loss = 0.00235640
    Iteration 124, loss = 0.00250213
    Iteration 125, loss = 0.00250526
    Iteration 126, loss = 0.00228289
    Iteration 127, loss = 0.00244699
    Iteration 128, loss = 0.00223582
    Iteration 129, loss = 0.00195448
    Iteration 130, loss = 0.00193931
    Iteration 131, loss = 0.00186323
    Iteration 132, loss = 0.00187579
    Iteration 133, loss = 0.00199710
    Iteration 134, loss = 0.00273512
    Iteration 135, loss = 0.00220252
    Iteration 136, loss = 0.00203277
    Iteration 137, loss = 0.00181513
    Iteration 138, loss = 0.00183299
    Iteration 139, loss = 0.00179257
    Iteration 140, loss = 0.00172292
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 2.36e+02 seconds
    Serialize model: 8.88e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 29-30

We can now simulate a torque-free attitude profile to inspect the quality of the fit

.. GENERATED FROM PYTHON SOURCE LINES 30-41

.. code-block:: default

    t_eval = np.linspace(0, 10, 1000)
    q, _ = ps.propagate_attitude_torque_free(
        np.array([0.0, 0.0, 0.0, 1.0]),
        np.array([1.0, 1.0, 1.0]),
        np.diag([1, 2, 3]),
        t_eval,
    )
    dcm = ps.quat_to_dcm(q)
    ovb = ps.stack_mat_mult(dcm, np.array([[1, 0, 0]]))
    svb = ps.stack_mat_mult(dcm, np.array([[0, 1, 0]]))








.. GENERATED FROM PYTHON SOURCE LINES 42-43

Evaluating the model in its two available formats - as a native ``scikit-learn`` model and as an Open Neural Network eXchange (ONNX) model

.. GENERATED FROM PYTHON SOURCE LINES 43-50

.. code-block:: default

    ps.tic("Evaluate trained model with sklearn")
    mdl_b_sklearn = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    ps.toc()
    ps.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    ps.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate trained model with sklearn: 1.56e-01 seconds
    Evaluate trained model with onnx: 6.31e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 51-52

We can save both of these representations to file:

.. GENERATED FROM PYTHON SOURCE LINES 52-55

.. code-block:: default

    mlp_bm.save_to_file(save_as_format="onnx")
    mlp_bm.save_to_file(save_as_format="sklearn")








.. GENERATED FROM PYTHON SOURCE LINES 56-57

Now we load the model from its ``.onxx`` file we just saved and evaluate the brightness

.. GENERATED FROM PYTHON SOURCE LINES 57-62

.. code-block:: default

    mlp_bm.load_from_file(mlp_bm.onnx_file_name)
    ps.tic("Evaluate loaded model with onxx")
    mdl_onnx_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    ps.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with onxx: 1.54e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 63-64

And we do the same for the ``scikit-learn`` ``.plk`` file we saved

.. GENERATED FROM PYTHON SOURCE LINES 64-69

.. code-block:: default

    mlp_bm.load_from_file(mlp_bm.sklearn_file_name)
    ps.tic("Evaluate loaded model with sklearn")
    mdl_sklearn_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    ps.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with sklearn: 1.34e-01 seconds




.. GENERATED FROM PYTHON SOURCE LINES 70-71

We can easily confirm that all four model evaluations have produced the same prediction

.. GENERATED FROM PYTHON SOURCE LINES 71-76

.. code-block:: default

    print(np.max(np.abs(mdl_b_sklearn - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_sklearn - mdl_sklearn_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_sklearn_loaded)))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    1.1707782774372788e-06
    0.0
    0.0
    1.1707782774372788e-06




.. GENERATED FROM PYTHON SOURCE LINES 77-78

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 78-91

.. code-block:: default

    true_b = obj.compute_convex_light_curve(brdf, ovb, svb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_sklearn, errorbar=None)
    plt.title(
        f"Light Curves for {obj.file_name}, {num_train} Training Points"
    )
    plt.xlabel("Time [s]")
    plt.ylabel("Normalized brightness")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()



.. image-sg:: /gallery/07-summer-2023/images/sphx_glr_rbf_fitting_001.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/07-summer-2023/images/sphx_glr_rbf_fitting_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 3 minutes  56.500 seconds)


.. _sphx_glr_download_gallery_07-summer-2023_rbf_fitting.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: rbf_fitting.py <rbf_fitting.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: rbf_fitting.ipynb <rbf_fitting.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
