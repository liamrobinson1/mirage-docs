
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/07-summer-2023/rbf_fitting.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_07-summer-2023_rbf_fitting.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_07-summer-2023_rbf_fitting.py:


Neural Network Brightness
=========================

Trains a neural network to predict the brightness of a specular cube in an arbitrary lighting and observation conditions and compares the results to the truth

.. GENERATED FROM PYTHON SOURCE LINES 7-19

.. code-block:: default



    import sys

    sys.path.append(".")

    import numpy as np
    import pyspaceaware as ps
    import pyspaceaware.sim as pssim
    import matplotlib.pyplot as plt
    import seaborn as sns








.. GENERATED FROM PYTHON SOURCE LINES 20-21

Let's define the object and the BRDF

.. GENERATED FROM PYTHON SOURCE LINES 21-23

.. code-block:: default

    obj = ps.SpaceObject("cube.obj")
    brdf = ps.Brdf("phong", cd=0.5, cs=0.5, n=10)







.. GENERATED FROM PYTHON SOURCE LINES 24-25

We now define the Multi-Layer Perceptron (MLP) brightness model. Note that the ``layers=(150, 50, 150)`` keyword argument defines the number of neurons in each densely-connected layer.

.. GENERATED FROM PYTHON SOURCE LINES 25-26

.. code-block:: default

    mlp_bm = pssim.MLPBrightnessModel(obj, brdf, use_engine=False, train_on="irradiance")







.. GENERATED FROM PYTHON SOURCE LINES 27-28

Now we train the model on a set number of training lighting and observation configurations. Usually ``1e5``-``1e6`` are required for a *good* fit

.. GENERATED FROM PYTHON SOURCE LINES 28-31

.. code-block:: default

    num_train = int(1e3)
    mlp_bm.train(num_train)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 1.04e-03 seconds
    (1024, 6) (1024,)
    Iteration 1, loss = 0.18095249
    Iteration 2, loss = 0.14640666
    Iteration 3, loss = 0.14005882
    Iteration 4, loss = 0.13225562
    Iteration 5, loss = 0.12836644
    Iteration 6, loss = 0.12449848
    Iteration 7, loss = 0.12071439
    Iteration 8, loss = 0.11913433
    Iteration 9, loss = 0.11584764
    Iteration 10, loss = 0.11336659
    Iteration 11, loss = 0.11001032
    Iteration 12, loss = 0.10720550
    Iteration 13, loss = 0.10534705
    Iteration 14, loss = 0.10194273
    Iteration 15, loss = 0.10076958
    Iteration 16, loss = 0.09751841
    Iteration 17, loss = 0.09418561
    Iteration 18, loss = 0.09249793
    Iteration 19, loss = 0.08848264
    Iteration 20, loss = 0.08523206
    Iteration 21, loss = 0.08159767
    Iteration 22, loss = 0.07854724
    Iteration 23, loss = 0.07490195
    Iteration 24, loss = 0.07267357
    Iteration 25, loss = 0.06975110
    Iteration 26, loss = 0.06469218
    Iteration 27, loss = 0.06001270
    Iteration 28, loss = 0.05934727
    Iteration 29, loss = 0.05553069
    Iteration 30, loss = 0.05186045
    Iteration 31, loss = 0.04907514
    Iteration 32, loss = 0.04725548
    Iteration 33, loss = 0.04681777
    Iteration 34, loss = 0.04508149
    Iteration 35, loss = 0.04283801
    Iteration 36, loss = 0.04086024
    Iteration 37, loss = 0.03788387
    Iteration 38, loss = 0.03578739
    Iteration 39, loss = 0.03530624
    Iteration 40, loss = 0.03376498
    Iteration 41, loss = 0.03328052
    Iteration 42, loss = 0.03679195
    Iteration 43, loss = 0.03219176
    Iteration 44, loss = 0.02991238
    Iteration 45, loss = 0.02861791
    Iteration 46, loss = 0.02730108
    Iteration 47, loss = 0.02692169
    Iteration 48, loss = 0.02723644
    Iteration 49, loss = 0.02524807
    Iteration 50, loss = 0.02445508
    Iteration 51, loss = 0.02485191
    Iteration 52, loss = 0.02250715
    Iteration 53, loss = 0.02166788
    Iteration 54, loss = 0.02026530
    Iteration 55, loss = 0.02142712
    Iteration 56, loss = 0.01910512
    Iteration 57, loss = 0.01823218
    Iteration 58, loss = 0.01894662
    Iteration 59, loss = 0.01759952
    Iteration 60, loss = 0.01689946
    Iteration 61, loss = 0.01809555
    Iteration 62, loss = 0.01792591
    Iteration 63, loss = 0.01725627
    Iteration 64, loss = 0.01738492
    Iteration 65, loss = 0.01438838
    Iteration 66, loss = 0.01327804
    Iteration 67, loss = 0.01325182
    Iteration 68, loss = 0.01480330
    Iteration 69, loss = 0.01354486
    Iteration 70, loss = 0.01347507
    Iteration 71, loss = 0.01188523
    Iteration 72, loss = 0.01119953
    Iteration 73, loss = 0.01144304
    Iteration 74, loss = 0.01135139
    Iteration 75, loss = 0.01042723
    Iteration 76, loss = 0.01025823
    Iteration 77, loss = 0.00997638
    Iteration 78, loss = 0.00927968
    Iteration 79, loss = 0.00883671
    Iteration 80, loss = 0.00877693
    Iteration 81, loss = 0.00833386
    Iteration 82, loss = 0.00828652
    Iteration 83, loss = 0.00883547
    Iteration 84, loss = 0.00914050
    Iteration 85, loss = 0.00841608
    Iteration 86, loss = 0.00793634
    Iteration 87, loss = 0.00708725
    Iteration 88, loss = 0.00666650
    Iteration 89, loss = 0.00653728
    Iteration 90, loss = 0.00637530
    Iteration 91, loss = 0.00628998
    Iteration 92, loss = 0.00651358
    Iteration 93, loss = 0.00596042
    Iteration 94, loss = 0.00624294
    Iteration 95, loss = 0.00624367
    Iteration 96, loss = 0.00565770
    Iteration 97, loss = 0.00512091
    Iteration 98, loss = 0.00523370
    Iteration 99, loss = 0.00504340
    Iteration 100, loss = 0.00549373
    Iteration 101, loss = 0.00513092
    Iteration 102, loss = 0.00478389
    Iteration 103, loss = 0.00484590
    Iteration 104, loss = 0.00435374
    Iteration 105, loss = 0.00429958
    Iteration 106, loss = 0.00405772
    Iteration 107, loss = 0.00395370
    Iteration 108, loss = 0.00376955
    Iteration 109, loss = 0.00392229
    Iteration 110, loss = 0.00397361
    Iteration 111, loss = 0.00402111
    Iteration 112, loss = 0.00384727
    Iteration 113, loss = 0.00384327
    Iteration 114, loss = 0.00365110
    Iteration 115, loss = 0.00351652
    Iteration 116, loss = 0.00324856
    Iteration 117, loss = 0.00321488
    Iteration 118, loss = 0.00309184
    Iteration 119, loss = 0.00337832
    Iteration 120, loss = 0.00323078
    Iteration 121, loss = 0.00322610
    Iteration 122, loss = 0.00295040
    Iteration 123, loss = 0.00294284
    Iteration 124, loss = 0.00303346
    Iteration 125, loss = 0.00288025
    Iteration 126, loss = 0.00277306
    Iteration 127, loss = 0.00280407
    Iteration 128, loss = 0.00296272
    Iteration 129, loss = 0.00285207
    Iteration 130, loss = 0.00255526
    Iteration 131, loss = 0.00251801
    Iteration 132, loss = 0.00241840
    Iteration 133, loss = 0.00229502
    Iteration 134, loss = 0.00226048
    Iteration 135, loss = 0.00229330
    Iteration 136, loss = 0.00215994
    Iteration 137, loss = 0.00206468
    Iteration 138, loss = 0.00217153
    Iteration 139, loss = 0.00198404
    Iteration 140, loss = 0.00205371
    Iteration 141, loss = 0.00207161
    Iteration 142, loss = 0.00203983
    Iteration 143, loss = 0.00217962
    Iteration 144, loss = 0.00232694
    Iteration 145, loss = 0.00262853
    Iteration 146, loss = 0.00229988
    Iteration 147, loss = 0.00218153
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 3.38e+00 seconds
    Serialize model: 7.19e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 32-33

We can now simulate a torque-free attitude profile to inspect the quality of the fit

.. GENERATED FROM PYTHON SOURCE LINES 33-44

.. code-block:: default

    t_eval = np.linspace(0, 10, 1000)
    q, _ = ps.propagate_attitude_torque_free(
        np.array([0.0, 0.0, 0.0, 1.0]),
        np.array([1.0, 1.0, 1.0]),
        np.diag([1, 2, 3]),
        t_eval,
    )
    dcm = ps.quat_to_dcm(q)
    ovb = ps.stack_mat_mult(dcm, np.array([[1, 0, 0]]))
    svb = ps.stack_mat_mult(dcm, np.array([[0, 1, 0]]))








.. GENERATED FROM PYTHON SOURCE LINES 45-46

Evaluating the model in its two available formats - as a native ``scikit-learn`` model and as an Open Neural Network eXchange (ONNX) model

.. GENERATED FROM PYTHON SOURCE LINES 46-53

.. code-block:: default

    ps.tic("Evaluate trained model with sklearn")
    mdl_b_sklearn = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    ps.toc()
    ps.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    ps.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate trained model with sklearn: 6.70e-03 seconds
    Evaluate trained model with onnx: 1.58e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 54-55

We can save both of these representations to file:

.. GENERATED FROM PYTHON SOURCE LINES 55-58

.. code-block:: default

    mlp_bm.save_to_file(save_as_format="onnx")
    mlp_bm.save_to_file(save_as_format="sklearn")








.. GENERATED FROM PYTHON SOURCE LINES 59-60

Now we load the model from its ``.onxx`` file we just saved and evaluate the brightness

.. GENERATED FROM PYTHON SOURCE LINES 60-65

.. code-block:: default

    mlp_bm.load_from_file(mlp_bm.onnx_file_name)
    ps.tic("Evaluate loaded model with onxx")
    mdl_onnx_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="onnx")
    ps.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with onxx: 1.61e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 66-67

And we do the same for the ``scikit-learn`` ``.plk`` file we saved

.. GENERATED FROM PYTHON SOURCE LINES 67-72

.. code-block:: default

    mlp_bm.load_from_file(mlp_bm.sklearn_file_name)
    ps.tic("Evaluate loaded model with sklearn")
    mdl_sklearn_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref="sklearn")
    ps.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with sklearn: 3.22e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 73-74

We can easily confirm that all four model evaluations have produced the same prediction

.. GENERATED FROM PYTHON SOURCE LINES 74-79

.. code-block:: default

    print(np.max(np.abs(mdl_b_sklearn - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_sklearn - mdl_sklearn_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_sklearn_loaded)))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    1.5996596340883684e-06
    0.0
    0.0
    1.5996596340883684e-06




.. GENERATED FROM PYTHON SOURCE LINES 80-81

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 81-93

.. code-block:: default

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_sklearn, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {num_train} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Normalized brightness")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()




.. image-sg:: /gallery/07-summer-2023/images/sphx_glr_rbf_fitting_001.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/07-summer-2023/images/sphx_glr_rbf_fitting_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 94-95

We can also train on magnitude data instead of irradiance:

.. GENERATED FROM PYTHON SOURCE LINES 95-102

.. code-block:: default

    mlp_bm = pssim.MLPBrightnessModel(obj, brdf, use_engine=True, train_on="magnitude")
    mlp_bm.train(num_train)

    ps.tic("Evaluate trained model with onnx")
    mdl_b_onnx = mlp_bm.eval(ovb, svb)
    ps.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Running Light Curve Engine: 
    /Users/liamrobinson/Documents/PyLightCurves/pyspaceaware/ctools/engine/LightCurveEngine -m /Users/liamrobinson/Documents/PyLightCurves/pyspaceaware/resources/models/cube.obj -i 9 -c 1024 -r light_curve0.lcr -x /Users/liamrobinson/Documents/PyLightCurves/examples/07-summer-2023/out  -b 1 -D 0.5 -S 0.5 -N 10

    Compute training LC: 6.71e-01 seconds
    (1024, 6) (1024,)
    Iteration 1, loss = 133.97533708
    Iteration 2, loss = 121.30508334
    Iteration 3, loss = 103.60609111
    Iteration 4, loss = 78.01354754
    Iteration 5, loss = 45.42295229
    Iteration 6, loss = 15.80479231
    Iteration 7, loss = 10.30363829
    Iteration 8, loss = 13.27971339
    Iteration 9, loss = 7.95553930
    Iteration 10, loss = 7.56227465
    Iteration 11, loss = 7.44747362
    Iteration 12, loss = 6.54939671
    Iteration 13, loss = 6.43897739
    Iteration 14, loss = 6.19737931
    Iteration 15, loss = 5.96044432
    Iteration 16, loss = 5.83583662
    Iteration 17, loss = 5.67359716
    Iteration 18, loss = 5.55008371
    Iteration 19, loss = 5.47288303
    Iteration 20, loss = 5.38651666
    Iteration 21, loss = 5.31713563
    Iteration 22, loss = 5.24218978
    Iteration 23, loss = 5.18508121
    Iteration 24, loss = 5.15380893
    Iteration 25, loss = 5.10261614
    Iteration 26, loss = 5.05773356
    Iteration 27, loss = 5.04182273
    Iteration 28, loss = 5.00045094
    Iteration 29, loss = 4.97101445
    Iteration 30, loss = 4.96239999
    Iteration 31, loss = 4.93497581
    Iteration 32, loss = 4.91766831
    Iteration 33, loss = 4.89861693
    Iteration 34, loss = 4.85736085
    Iteration 35, loss = 4.84559266
    Iteration 36, loss = 4.84512654
    Iteration 37, loss = 4.83177970
    Iteration 38, loss = 4.79710908
    Iteration 39, loss = 4.79168075
    Iteration 40, loss = 4.77897808
    Iteration 41, loss = 4.76918155
    Iteration 42, loss = 4.74932468
    Iteration 43, loss = 4.72668818
    Iteration 44, loss = 4.71666442
    Iteration 45, loss = 4.70558476
    Iteration 46, loss = 4.69581582
    Iteration 47, loss = 4.69073521
    Iteration 48, loss = 4.69079199
    Iteration 49, loss = 4.65408189
    Iteration 50, loss = 4.65887126
    Iteration 51, loss = 4.65385016
    Iteration 52, loss = 4.63430897
    Iteration 53, loss = 4.61479573
    Iteration 54, loss = 4.60154413
    Iteration 55, loss = 4.58921984
    Iteration 56, loss = 4.57365679
    Iteration 57, loss = 4.57601688
    Iteration 58, loss = 4.55404573
    Iteration 59, loss = 4.54307249
    Iteration 60, loss = 4.53711308
    Iteration 61, loss = 4.51914496
    Iteration 62, loss = 4.50203995
    Iteration 63, loss = 4.49542370
    Iteration 64, loss = 4.49300920
    Iteration 65, loss = 4.48090940
    Iteration 66, loss = 4.45586888
    Iteration 67, loss = 4.44473785
    Iteration 68, loss = 4.44060888
    Iteration 69, loss = 4.41491834
    Iteration 70, loss = 4.41021105
    Iteration 71, loss = 4.39965782
    Iteration 72, loss = 4.38824682
    Iteration 73, loss = 4.42293537
    Iteration 74, loss = 4.37417282
    Iteration 75, loss = 4.41660045
    Iteration 76, loss = 4.36710238
    Iteration 77, loss = 4.35049985
    Iteration 78, loss = 4.32853967
    Iteration 79, loss = 4.32434065
    Iteration 80, loss = 4.30735071
    Iteration 81, loss = 4.29137524
    Iteration 82, loss = 4.30552492
    Iteration 83, loss = 4.31725042
    Iteration 84, loss = 4.29235765
    Iteration 85, loss = 4.25104920
    Iteration 86, loss = 4.24989681
    Iteration 87, loss = 4.22963490
    Iteration 88, loss = 4.23130031
    Iteration 89, loss = 4.23319052
    Iteration 90, loss = 4.21757662
    Iteration 91, loss = 4.19501872
    Iteration 92, loss = 4.19502125
    Iteration 93, loss = 4.15590416
    Iteration 94, loss = 4.15108990
    Iteration 95, loss = 4.12219648
    Iteration 96, loss = 4.17152915
    Iteration 97, loss = 4.10661034
    Iteration 98, loss = 4.08214747
    Iteration 99, loss = 4.10879395
    Iteration 100, loss = 4.05983926
    Iteration 101, loss = 4.07150498
    Iteration 102, loss = 4.05173375
    Iteration 103, loss = 4.02291087
    Iteration 104, loss = 4.00427372
    Iteration 105, loss = 3.99121090
    Iteration 106, loss = 3.97570210
    Iteration 107, loss = 3.95759826
    Iteration 108, loss = 3.96442648
    Iteration 109, loss = 3.94338434
    Iteration 110, loss = 3.92189776
    Iteration 111, loss = 3.89425454
    Iteration 112, loss = 3.87833271
    Iteration 113, loss = 3.84914969
    Iteration 114, loss = 3.83807349
    Iteration 115, loss = 3.83638119
    Iteration 116, loss = 3.78631785
    Iteration 117, loss = 3.76367034
    Iteration 118, loss = 3.78321566
    Iteration 119, loss = 3.77205568
    Iteration 120, loss = 3.74524154
    Iteration 121, loss = 3.68813077
    Iteration 122, loss = 3.73918539
    Iteration 123, loss = 3.69870192
    Iteration 124, loss = 3.66344300
    Iteration 125, loss = 3.61101086
    Iteration 126, loss = 3.60252218
    Iteration 127, loss = 3.56788880
    Iteration 128, loss = 3.54830742
    Iteration 129, loss = 3.53283829
    Iteration 130, loss = 3.52991013
    Iteration 131, loss = 3.47224007
    Iteration 132, loss = 3.47364415
    Iteration 133, loss = 3.43016783
    Iteration 134, loss = 3.42899211
    Iteration 135, loss = 3.38850848
    Iteration 136, loss = 3.36963984
    Iteration 137, loss = 3.38665478
    Iteration 138, loss = 3.33738244
    Iteration 139, loss = 3.31504560
    Iteration 140, loss = 3.32499004
    Iteration 141, loss = 3.27387919
    Iteration 142, loss = 3.26832378
    Iteration 143, loss = 3.29674735
    Iteration 144, loss = 3.26453786
    Iteration 145, loss = 3.22029019
    Iteration 146, loss = 3.18547526
    Iteration 147, loss = 3.14942827
    Iteration 148, loss = 3.13167669
    Iteration 149, loss = 3.13044477
    Iteration 150, loss = 3.11679265
    Iteration 151, loss = 3.10022521
    Iteration 152, loss = 3.05407645
    Iteration 153, loss = 3.02694512
    Iteration 154, loss = 3.00746350
    Iteration 155, loss = 3.03609741
    Iteration 156, loss = 3.06298064
    Iteration 157, loss = 3.03867889
    Iteration 158, loss = 2.95329137
    Iteration 159, loss = 2.92412077
    Iteration 160, loss = 2.89206477
    Iteration 161, loss = 2.89706183
    Iteration 162, loss = 2.94536844
    Iteration 163, loss = 2.92860363
    Iteration 164, loss = 2.83014350
    Iteration 165, loss = 2.81557194
    Iteration 166, loss = 2.77519477
    Iteration 167, loss = 2.75830526
    Iteration 168, loss = 2.75552653
    Iteration 169, loss = 2.71864363
    Iteration 170, loss = 2.77027142
    Iteration 171, loss = 2.70971107
    Iteration 172, loss = 2.67782508
    Iteration 173, loss = 2.62797203
    Iteration 174, loss = 2.60492389
    Iteration 175, loss = 2.59062499
    Iteration 176, loss = 2.57284195
    Iteration 177, loss = 2.54147908
    Iteration 178, loss = 2.54075740
    Iteration 179, loss = 2.48232354
    Iteration 180, loss = 2.48409895
    Iteration 181, loss = 2.46835260
    Iteration 182, loss = 2.55667135
    Iteration 183, loss = 2.51779866
    Iteration 184, loss = 2.43978052
    Iteration 185, loss = 2.39911192
    Iteration 186, loss = 2.37991175
    Iteration 187, loss = 2.35260950
    Iteration 188, loss = 2.33535643
    Iteration 189, loss = 2.32663316
    Iteration 190, loss = 2.27930341
    Iteration 191, loss = 2.27213910
    Iteration 192, loss = 2.24413448
    Iteration 193, loss = 2.27355180
    Iteration 194, loss = 2.23343623
    Iteration 195, loss = 2.20419765
    Iteration 196, loss = 2.14777425
    Iteration 197, loss = 2.15785776
    Iteration 198, loss = 2.15658907
    Iteration 199, loss = 2.13793203
    Iteration 200, loss = 2.10468701
    /Users/liamrobinson/Documents/PyLightCurves/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
      warnings.warn(
    Fit against 1000 pts: : 4.77e+00 seconds
    Serialize model: 4.76e-02 seconds
    Evaluate trained model with onnx: 4.42e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 103-104

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 104-115

.. code-block:: default

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_onnx, errorbar=None)
    plt.title(f"Light Curves for {obj.file_name}, {int(1e6)} Training Points")
    plt.xlabel("Time [s]")
    plt.ylabel("Apparent Magnitude")
    plt.legend(["True", "Model"])
    plt.grid()
    plt.show()



.. image-sg:: /gallery/07-summer-2023/images/sphx_glr_rbf_fitting_002.png
   :alt: Light Curves for cube.obj, 1000000 Training Points
   :srcset: /gallery/07-summer-2023/images/sphx_glr_rbf_fitting_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Running Light Curve Engine: 
    /Users/liamrobinson/Documents/PyLightCurves/pyspaceaware/ctools/engine/LightCurveEngine -m /Users/liamrobinson/Documents/PyLightCurves/pyspaceaware/resources/models/cube.obj -i 9 -c 1000 -r light_curve0.lcr -x /Users/liamrobinson/Documents/PyLightCurves/examples/07-summer-2023/out  -b 1 -D 0.5 -S 0.5 -N 10






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  10.320 seconds)


.. _sphx_glr_download_gallery_07-summer-2023_rbf_fitting.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: rbf_fitting.py <rbf_fitting.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: rbf_fitting.ipynb <rbf_fitting.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
